{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c1efbe",
   "metadata": {},
   "source": [
    "1. PTQ Static with calibration MinMax (per-tensor and per-channel via fbgemm)\n",
    "2. CNN, CNN with Early Exit and Pre-trained models (MobileNetV2, ResNet50, ResNeXt50, EfficientNet-B0)\n",
    "3. Dataset: CIFAR-10, CIFAR-100, Tiny ImageNet\n",
    "4. Target: CPU inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fee44c",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a97524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Literal, Dict, Any, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.ao.quantization\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.quantization import QuantStub, DeQuantStub, prepare_qat, convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a24490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cpu\n",
      "['none', 'onednn', 'x86', 'fbgemm']\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(torch.backends.quantized.supported_engines)\n",
    "device  = torch.device(\"cpu\")\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795056a",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68d85b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetName = Literal[\"cifar10\", \"cifar100\", \"tiny_imagenet\", \"mnist\"]\n",
    "\n",
    "class DataloaderManager:\n",
    "    CONFIGS = {\n",
    "        \"cifar10\": {\n",
    "            \"size\": 32, \"padding\": 4, \"num_classes\": 10,\n",
    "            \"stats\": ((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "        },\n",
    "        \"cifar100\": {\n",
    "            \"size\": 32, \"padding\": 4, \"num_classes\": 100,\n",
    "            \"stats\": ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "        },\n",
    "        \"tiny_imagenet\": {\n",
    "            \"size\": 64, \"padding\": 8, \"num_classes\": 200,\n",
    "            \"stats\": ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        },\n",
    "        \"mnist\": {\n",
    "            \"size\": 28, \"padding\": 0, \"num_classes\": 10,\n",
    "            \"stats\": ((0.1307,), (0.3081,))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, root_dir: str, dataset: DatasetName):\n",
    "        self.root = Path(root_dir)\n",
    "        self.dataset = dataset\n",
    "        self.cfg = self.CONFIGS[dataset]\n",
    "        \n",
    "    def _get_transforms(self, train: bool):\n",
    "        mean, std = self.cfg[\"stats\"]\n",
    "        tf_list = []\n",
    "        if train:\n",
    "            if self.dataset != \"mnist\":\n",
    "                tf_list.extend([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomCrop(self.cfg[\"size\"], padding=self.cfg[\"padding\"])\n",
    "                ])\n",
    "            else:\n",
    "                tf_list.append(transforms.RandomRotation(10))\n",
    "\n",
    "        tf_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "        return transforms.Compose(tf_list)\n",
    "\n",
    "    def _get_dataset_instance(self, train: bool):\n",
    "        tf = self._get_transforms(train)\n",
    "\n",
    "        if self.dataset.startswith(\"cifar\"):\n",
    "            ds_cls = datasets.CIFAR10 if self.dataset == \"cifar10\" else datasets.CIFAR100\n",
    "            return ds_cls(self.root, train=train, download=True, transform=tf)\n",
    "\n",
    "        if self.dataset == \"mnist\":\n",
    "            return datasets.MNIST(self.root, train=train, download=True, transform=tf)\n",
    "\n",
    "        path = self.root / \"tiny-imagenet-200\" / (\"train\" if train else \"val\")\n",
    "        return datasets.ImageFolder(str(path), transform=tf)\n",
    "\n",
    "    def get_loaders(self, batch_size: int, num_workers: int):\n",
    "        train_ds = self._get_dataset_instance(train=True)\n",
    "        val_ds   = self._get_dataset_instance(train=False)\n",
    "\n",
    "        loader_args = {\"batch_size\": batch_size, \"num_workers\": num_workers, \"pin_memory\": True}\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, **loader_args)\n",
    "        val_loader   = torch.utils.data.DataLoader(val_ds, shuffle=False, **loader_args)\n",
    "\n",
    "        self._print_summary(train_loader, val_loader, batch_size)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def _print_summary(self, train_loader, val_loader, batch_size):\n",
    "        print(f\"Dataset      : {self.dataset}\")\n",
    "        print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "        print(f\"Val samples  : {len(val_loader.dataset)}\")\n",
    "        print(f\"Classes      : {self.cfg['num_classes']}\")\n",
    "        print(f\"Batch size   : {batch_size}\")\n",
    "        print(f\"Train batches: {len(train_loader)}\")\n",
    "        print(f\"Val batches  : {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3faaf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset      : mnist\n",
      "Train samples: 60000\n",
      "Val samples  : 10000\n",
      "Classes      : 10\n",
      "Batch size   : 64\n",
      "Train batches: 938\n",
      "Val batches  : 157\n"
     ]
    }
   ],
   "source": [
    "manager = DataloaderManager(root_dir=\"../data\", dataset=\"mnist\")\n",
    "train_loader, val_loader = manager.get_loaders(batch_size=64, num_workers=4)\n",
    "num_classes = manager.cfg[\"num_classes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a4ddb",
   "metadata": {},
   "source": [
    "### Training and evaluation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a61061ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    device, \n",
    "    epochs: int, \n",
    "    lr: float, \n",
    "    model_name: str = \"model\"\n",
    ") -> nn.Module:\n",
    "    \n",
    "    print(f\"Training {model_name} | Epochs: {epochs} | LR: {lr}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{running_loss/len(train_loader):.4f}\")\n",
    "            \n",
    "        patience = 5\n",
    "        trigger_times = 0\n",
    "        acc = validate_model(model, val_loader, device, desc=f\"Eval Ep {epoch+1}\")\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Result Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f} | Acc = {acc:.2f}%\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), f\"best_{model_name}.pth\")\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"best_{model_name}.pth\", weights_only=True))\n",
    "    return model\n",
    "\n",
    "def validate_model(model: nn.Module, data_loader, device, desc=\"Validating\") -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(data_loader, desc=desc, leave=False):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "    return 100.0 * correct / total\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader, device) -> float:\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            pred    = model(data).argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total   += target.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "def measure_inference_time(model: nn.Module, test_loader, device, num_batches: int = 20) -> float:\n",
    "    \"\"\"Return mean latency per batch in milliseconds.\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            if i < 3:   # warmup\n",
    "                _ = model(data)\n",
    "                continue\n",
    "            t0 = time.time()\n",
    "            _  = model(data)\n",
    "            times.append(time.time() - t0)\n",
    "    return sum(times) / len(times) * 1000\n",
    "\n",
    "def get_model_size(model: nn.Module) -> float:\n",
    "    buf = io.BytesIO()\n",
    "    torch.save(model.state_dict(), buf)\n",
    "    return buf.tell() / 1024 / 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1e9fd",
   "metadata": {},
   "source": [
    "# PTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd12e2",
   "metadata": {},
   "source": [
    "## Calibration MinMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b52e9",
   "metadata": {},
   "source": [
    "### Experiment 1 - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92049c3",
   "metadata": {},
   "source": [
    "#### MLP without observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3e7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # [batch, 3, 32, 32] → [batch, 3072] === nn.Linear(3072, 512)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# REDE MAIS PROFUNDA\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "#             nn.BatchNorm1d(hidden_dim // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden_dim // 2, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acbe3c4",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffcf010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training experiment1_model_fp32 | Epochs: 5 | LR: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 938/938 [00:07<00:00, 125.69it/s, loss=0.2811]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 1: Loss = 0.2811 | Acc = 96.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 938/938 [00:07<00:00, 126.34it/s, loss=0.1198]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 2: Loss = 0.1198 | Acc = 97.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 938/938 [00:07<00:00, 125.94it/s, loss=0.0863]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 3: Loss = 0.0863 | Acc = 97.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 938/938 [00:07<00:00, 128.16it/s, loss=0.0693]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 4: Loss = 0.0693 | Acc = 97.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 938/938 [00:07<00:00, 129.58it/s, loss=0.0609]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 5: Loss = 0.0609 | Acc = 98.12%\n",
      "Results:\n",
      "Acc     : 98.12%\n",
      "Latency : 0.24 ms/batch\n",
      "Size    : 1.55458 MB\n"
     ]
    }
   ],
   "source": [
    "# model_fp32 = MLP(input_dim=3072, hidden_dim=512, output_dim=10).to(device) # cifar10\n",
    "model_fp32 = MLP(input_dim=784, hidden_dim=512, output_dim=10).to(device)\n",
    "model_fp32 = train_model(\n",
    "    model_fp32, train_loader, val_loader, device,\n",
    "    epochs=5, lr=0.01, model_name=\"experiment1_model_fp32\"\n",
    ")\n",
    "\n",
    "acc_model_fp32  = evaluate_model(model_fp32, val_loader, device)\n",
    "time_model_fp32 = measure_inference_time(model_fp32, val_loader, device)\n",
    "size_model_fp32 = get_model_size(model_fp32)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"Acc     : {acc_model_fp32:.2f}%\")\n",
    "print(f\"Latency : {time_model_fp32:.2f} ms/batch\")\n",
    "print(f\"Size    : {size_model_fp32:.5f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0590479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Structure:\n",
      "linear1.weight -> torch.Size([512, 784])\n",
      "linear1.bias -> torch.Size([512])\n",
      "linear2.weight -> torch.Size([10, 512])\n",
      "linear2.bias -> torch.Size([10])\n",
      "==========================================\n",
      "Weights Statistics:\n",
      "\n",
      "linear1.weight:\n",
      "tensor([-0.0175,  0.0201, -0.0092,  0.0005, -0.0101, -0.0255,  0.0081, -0.0065,\n",
      "        -0.0162, -0.0012])\n",
      "Dtype: torch.float32\n",
      "Shape: (512, 784)\n",
      "Mean: -0.000682\n",
      "Std:  0.022832\n",
      "------------------------------------------\n",
      "linear1.bias:\n",
      "tensor([ 0.0114, -0.0190,  0.0070, -0.0084,  0.0045,  0.0068, -0.0315, -0.0023,\n",
      "         0.0089,  0.0088])\n",
      "Dtype: torch.float32\n",
      "Shape: (512,)\n",
      "Mean: 0.001357\n",
      "Std:  0.020073\n",
      "------------------------------------------\n",
      "linear2.weight:\n",
      "tensor([-0.0865,  0.1052,  0.0611, -0.1032, -0.0705,  0.1378, -0.0799,  0.0163,\n",
      "        -0.1358,  0.0837])\n",
      "Dtype: torch.float32\n",
      "Shape: (10, 512)\n",
      "Mean: 0.000050\n",
      "Std:  0.085414\n",
      "------------------------------------------\n",
      "linear2.bias:\n",
      "tensor([-0.0715, -0.0490, -0.0043, -0.0023,  0.0380,  0.0446, -0.0464, -0.0715,\n",
      "         0.1086, -0.0098])\n",
      "Dtype: torch.float32\n",
      "Shape: (10,)\n",
      "Mean: -0.006361\n",
      "Std:  0.057474\n",
      "------------------------------------------\n",
      "Datatype model:\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"best_experiment1_model_fp32.pth\", map_location=\"cpu\", weights_only=True)\n",
    "model_fp32.load_state_dict(state_dict)\n",
    "model_fp32.eval()\n",
    "\n",
    "print(\"Weights Structure:\")\n",
    "for name, param in model_fp32.named_parameters():\n",
    "    print(f\"{name} -> {param.shape}\")\n",
    "    \n",
    "print('='*42)\n",
    "\n",
    "print(\"Weights Statistics:\\n\")\n",
    "\n",
    "for name, param in model_fp32.named_parameters():\n",
    "    print(f\"{name}:\")\n",
    "    print(param.data.view(-1)[:10])\n",
    "    print(f\"Dtype: {param.dtype}\")\n",
    "    print(f\"Shape: {tuple(param.shape)}\")\n",
    "    print(f\"Mean: {param.data.mean().item():.6f}\")\n",
    "    print(f\"Std:  {param.data.std().item():.6f}\")\n",
    "    print('-'*42)\n",
    "  \n",
    "print('Datatype model:')  \n",
    "print(next(model_fp32.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e6d58",
   "metadata": {},
   "source": [
    "#### MLP with observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3afd7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad341859",
   "metadata": {},
   "source": [
    "#### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f67f4ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=512, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(\n",
       "    in_features=512, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prepared = QuantizedMLP(input_dim=784, hidden_dim=512, output_dim=10).to(device)\n",
    "model_prepared.load_state_dict(model_fp32.state_dict())\n",
    "model_prepared.eval()\n",
    "\n",
    "model_prepared.qconfig = torch.ao.quantization.default_qconfig\n",
    "model_prepared = torch.ao.quantization.prepare(model_prepared)\n",
    "model_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37de3b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 Accuracy: 98.12%\n",
      "FP32 Latency:  0.73 ms/batch\n",
      "FP32 Size:  1.55711 MB\n"
     ]
    }
   ],
   "source": [
    "acc_model_prepared = validate_model(model_prepared, val_loader, device, desc='Val Quantized')\n",
    "lat_model_prepared = measure_inference_time(model_prepared, val_loader, device)\n",
    "model_size_model_prepared = get_model_size(model_prepared)\n",
    "\n",
    "print(f\"FP32 Accuracy: {acc_model_prepared:.2f}%\")\n",
    "print(f\"FP32 Latency:  {lat_model_prepared:.2f} ms/batch\")\n",
    "print(f\"FP32 Size:  {model_size_model_prepared:.5f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9bf03",
   "metadata": {},
   "source": [
    "#### Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f930785d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=512, scale=0.16171421110630035, zero_point=70, qscheme=torch.per_tensor_affine)\n",
       "  (relu): ReLU()\n",
       "  (linear2): QuantizedLinear(in_features=512, out_features=10, scale=0.31290170550346375, zero_point=52, qscheme=torch.per_tensor_affine)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prepared\n",
    "model_quantized = torch.ao.quantization.convert(model_prepared)\n",
    "model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f3d3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Statistics:\n",
      "\n",
      "tensor([[-15,  17,  -8,  ...,   7,  11, -17],\n",
      "        [-11,   8,  10,  ...,  24,  11,   4],\n",
      "        [ 24,  16,  16,  ...,  28, -21,  -8],\n",
      "        ...,\n",
      "        [-23, -14, -11,  ..., -26, -18,  22],\n",
      "        [ -9,  -9,  29,  ...,   3,  14,  -9],\n",
      "        [-26,  29, -27,  ...,  11, -22,  -7]], dtype=torch.int8)\n",
      "tensor([[-30,  37,  21,  ...,  23, -22, -27],\n",
      "        [ 17, -15, -38,  ..., -42,  25,  39],\n",
      "        [-71, -48, -48,  ...,   0,  -2, -43],\n",
      "        ...,\n",
      "        [  8,  18,  16,  ..., -60,  18, -15],\n",
      "        [ 14, -47,  33,  ...,  34, -11, -31],\n",
      "        [ 20,  24,  32,  ..., -18, -22,  13]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights Statistics:\\n\")\n",
    "print(torch.int_repr(model_quantized.linear1.weight()))\n",
    "print(torch.int_repr(model_quantized.linear2.weight()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de9190",
   "metadata": {},
   "source": [
    "#### Int8 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2b2d315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc INT8     : 98.10%\n",
      "Latency INT8 : 0.62 ms/batch\n",
      "Size INT8    : 0.39 MB\n"
     ]
    }
   ],
   "source": [
    "acc_model_quantized = validate_model(model_quantized, val_loader, device, desc='Final INT8 Eval')\n",
    "lat_model_quantized = measure_inference_time(model_quantized, val_loader, device)\n",
    "model_size_quantized = get_model_size(model_quantized)\n",
    "\n",
    "print(f\"Acc INT8     : {acc_model_quantized:.2f}%\")\n",
    "print(f\"Latency INT8 : {lat_model_quantized:.2f} ms/batch\")\n",
    "print(f\"Size INT8    : {model_size_quantized:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395411bd",
   "metadata": {},
   "source": [
    "| Type  | Acc | Latency | Size |\n",
    "| ----- | --- | ------- | ---- |\n",
    "| FP32  | 98.12% | 0.73 ms/batch | 1.55711 MB |\n",
    "| INT8  | 98.10% | 0.62 ms/batch | 0.39 MB    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503590f",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8daff85",
   "metadata": {},
   "source": [
    "#### CNN No Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc8b940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NO_OBS(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.fc   = nn.Linear(128 * 8 * 8, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c3c46",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f957804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training experiment2_cnn_fp32 | Epochs: 5 | LR: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 782/782 [00:30<00:00, 25.30it/s, loss=1.5273]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 1: Loss = 1.5273 | Acc = 64.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 782/782 [00:31<00:00, 24.71it/s, loss=1.0502]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 2: Loss = 1.0502 | Acc = 60.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 782/782 [00:31<00:00, 24.87it/s, loss=0.8913]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 3: Loss = 0.8913 | Acc = 71.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 782/782 [00:31<00:00, 24.69it/s, loss=0.7889]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 4: Loss = 0.7889 | Acc = 75.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 782/782 [00:31<00:00, 25.18it/s, loss=0.7156]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 5: Loss = 0.7156 | Acc = 76.63%\n",
      "Results:\n",
      "Acc     : 76.63%\n",
      "Latency : 8.63 ms/batch\n",
      "Size    : 0.67806 MB\n"
     ]
    }
   ],
   "source": [
    "cnn_fp32 = CNN_NO_OBS(num_classes=10).to(device)\n",
    "cnn_fp32 = train_model(\n",
    "    cnn_fp32, train_loader, val_loader, device,\n",
    "    epochs=5, lr=0.01, model_name=\"experiment2_cnn_fp32\"\n",
    ")\n",
    "\n",
    "acc_cnn_fp32  = evaluate_model(cnn_fp32, val_loader, device)\n",
    "time_cnn_fp32 = measure_inference_time(cnn_fp32, val_loader, device)\n",
    "size_cnn_fp32 = get_model_size(cnn_fp32)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"Acc     : {acc_cnn_fp32:.2f}%\")\n",
    "print(f\"Latency : {time_cnn_fp32:.2f} ms/batch\")\n",
    "print(f\"Size    : {size_cnn_fp32:.5f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d18566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Structure:\n",
      "conv1.weight -> torch.Size([32, 3, 3, 3])\n",
      "conv1.bias -> torch.Size([32])\n",
      "bn1.weight -> torch.Size([32])\n",
      "bn1.bias -> torch.Size([32])\n",
      "conv2.weight -> torch.Size([64, 32, 3, 3])\n",
      "conv2.bias -> torch.Size([64])\n",
      "bn2.weight -> torch.Size([64])\n",
      "bn2.bias -> torch.Size([64])\n",
      "conv3.weight -> torch.Size([128, 64, 3, 3])\n",
      "conv3.bias -> torch.Size([128])\n",
      "bn3.weight -> torch.Size([128])\n",
      "bn3.bias -> torch.Size([128])\n",
      "fc.weight -> torch.Size([10, 8192])\n",
      "fc.bias -> torch.Size([10])\n",
      "==========================================\n",
      "Weights Statistics:\n",
      "\n",
      "conv1.weight:\n",
      "tensor([ 0.0483,  0.0565,  0.1348, -0.1159, -0.1469, -0.1909,  0.1750, -0.2315,\n",
      "        -0.1932,  0.1487])\n",
      "Dtype: torch.float32\n",
      "Shape: (32, 3, 3, 3)\n",
      "Mean: 0.000287\n",
      "Std:  0.192709\n",
      "------------------------------------------\n",
      "conv1.bias:\n",
      "tensor([ 0.1551,  0.0933, -0.1757,  0.1327,  0.1873,  0.0394,  0.0653,  0.0367,\n",
      "        -0.0382, -0.1751])\n",
      "Dtype: torch.float32\n",
      "Shape: (32,)\n",
      "Mean: -0.006533\n",
      "Std:  0.101757\n",
      "------------------------------------------\n",
      "bn1.weight:\n",
      "tensor([0.9763, 0.9166, 0.9827, 1.0991, 1.0657, 1.0045, 0.9678, 0.9534, 1.1026,\n",
      "        1.0404])\n",
      "Dtype: torch.float32\n",
      "Shape: (32,)\n",
      "Mean: 0.971007\n",
      "Std:  0.111075\n",
      "------------------------------------------\n",
      "bn1.bias:\n",
      "tensor([-0.1910, -0.2197, -0.0236, -0.0390,  0.0472, -0.0615, -0.1265, -0.1738,\n",
      "        -0.0939, -0.0039])\n",
      "Dtype: torch.float32\n",
      "Shape: (32,)\n",
      "Mean: -0.053595\n",
      "Std:  0.096025\n",
      "------------------------------------------\n",
      "conv2.weight:\n",
      "tensor([ 0.0059,  0.0608, -0.0275, -0.0178,  0.0342, -0.0508,  0.0433, -0.0869,\n",
      "        -0.0165, -0.0177])\n",
      "Dtype: torch.float32\n",
      "Shape: (64, 32, 3, 3)\n",
      "Mean: -0.003041\n",
      "Std:  0.051218\n",
      "------------------------------------------\n",
      "conv2.bias:\n",
      "tensor([ 0.0014,  0.0367, -0.0503, -0.0392,  0.0283,  0.0521, -0.0307, -0.0346,\n",
      "         0.0453,  0.0364])\n",
      "Dtype: torch.float32\n",
      "Shape: (64,)\n",
      "Mean: 0.000668\n",
      "Std:  0.036872\n",
      "------------------------------------------\n",
      "bn2.weight:\n",
      "tensor([1.0016, 1.0619, 0.7925, 1.0951, 0.9664, 1.0141, 0.9558, 0.9338, 0.9893,\n",
      "        1.0850])\n",
      "Dtype: torch.float32\n",
      "Shape: (64,)\n",
      "Mean: 0.962564\n",
      "Std:  0.069442\n",
      "------------------------------------------\n",
      "bn2.bias:\n",
      "tensor([-0.0171, -0.1034, -0.1913, -0.1442, -0.0488, -0.2263, -0.0942, -0.1496,\n",
      "        -0.1625, -0.1524])\n",
      "Dtype: torch.float32\n",
      "Shape: (64,)\n",
      "Mean: -0.151326\n",
      "Std:  0.075503\n",
      "------------------------------------------\n",
      "conv3.weight:\n",
      "tensor([ 0.0238, -0.0200, -0.0222, -0.0026,  0.0050,  0.0035,  0.0148, -0.0194,\n",
      "         0.0269,  0.0421])\n",
      "Dtype: torch.float32\n",
      "Shape: (128, 64, 3, 3)\n",
      "Mean: -0.003644\n",
      "Std:  0.033593\n",
      "------------------------------------------\n",
      "conv3.bias:\n",
      "tensor([ 0.0310, -0.0387,  0.0333,  0.0262, -0.0338, -0.0264, -0.0059,  0.0294,\n",
      "         0.0138, -0.0145])\n",
      "Dtype: torch.float32\n",
      "Shape: (128,)\n",
      "Mean: 0.001653\n",
      "Std:  0.023705\n",
      "------------------------------------------\n",
      "bn3.weight:\n",
      "tensor([0.6776, 0.7597, 0.7166, 0.6994, 0.7081, 0.6781, 0.7132, 0.6524, 0.7221,\n",
      "        0.7563])\n",
      "Dtype: torch.float32\n",
      "Shape: (128,)\n",
      "Mean: 0.708780\n",
      "Std:  0.032015\n",
      "------------------------------------------\n",
      "bn3.bias:\n",
      "tensor([-0.4205, -0.4449, -0.4189, -0.4148, -0.3244, -0.4117, -0.4236, -0.4334,\n",
      "        -0.4438, -0.3993])\n",
      "Dtype: torch.float32\n",
      "Shape: (128,)\n",
      "Mean: -0.405964\n",
      "Std:  0.031751\n",
      "------------------------------------------\n",
      "fc.weight:\n",
      "tensor([ 0.0034,  0.0071, -0.0075, -0.0024,  0.0102,  0.0157, -0.0120,  0.0299,\n",
      "        -0.0083, -0.0059])\n",
      "Dtype: torch.float32\n",
      "Shape: (10, 8192)\n",
      "Mean: 0.000017\n",
      "Std:  0.027489\n",
      "------------------------------------------\n",
      "fc.bias:\n",
      "tensor([ 0.0438, -0.0652, -0.0484,  0.0350,  0.0400,  0.0213, -0.0386,  0.0190,\n",
      "        -0.0734,  0.0735])\n",
      "Dtype: torch.float32\n",
      "Shape: (10,)\n",
      "Mean: 0.000691\n",
      "Std:  0.052106\n",
      "------------------------------------------\n",
      "Datatype model:\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"best_experiment2_cnn_fp32.pth\", map_location=\"cpu\", weights_only=True)\n",
    "cnn_fp32.load_state_dict(state_dict)\n",
    "cnn_fp32.eval()\n",
    "\n",
    "print(\"Weights Structure:\")\n",
    "for name, param in cnn_fp32.named_parameters():\n",
    "    print(f\"{name} -> {param.shape}\")\n",
    "    \n",
    "print('='*42)\n",
    "\n",
    "print(\"Weights Statistics:\\n\")\n",
    "\n",
    "for name, param in cnn_fp32.named_parameters():\n",
    "    print(f\"{name}:\")\n",
    "    print(param.data.view(-1)[:10])\n",
    "    print(f\"Dtype: {param.dtype}\")\n",
    "    print(f\"Shape: {tuple(param.shape)}\")\n",
    "    print(f\"Mean: {param.data.mean().item():.6f}\")\n",
    "    print(f\"Std:  {param.data.std().item():.6f}\")\n",
    "    print('-'*42)\n",
    "  \n",
    "print('Datatype model:')  \n",
    "print(next(cnn_fp32.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a4a452",
   "metadata": {},
   "source": [
    "#### CNN Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b6d3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_OBS(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.quant   = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.fc   = nn.Linear(128 * 8 * 8, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.quant(x)\n",
    "\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    CNN_FUSION_PATTERNS = [\n",
    "        [\"conv1\", \"bn1\", \"relu1\"],\n",
    "        [\"conv2\", \"bn2\", \"relu2\"],\n",
    "        [\"conv3\", \"bn3\", \"relu3\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de97498",
   "metadata": {},
   "source": [
    "#### Folding BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6533a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_conv_bn_relu_sequences(module: nn.Module) -> List[List[str]]:\n",
    "    children = list(module.named_children())\n",
    "    patterns: List[List[str]] = []\n",
    "    i = 0\n",
    "    while i < len(children):\n",
    "        n0, m0 = children[i]\n",
    "        if isinstance(m0, nn.Conv2d) and i + 1 < len(children):\n",
    "            n1, m1 = children[i + 1]\n",
    "            if isinstance(m1, nn.BatchNorm2d):\n",
    "                if i + 2 < len(children):\n",
    "                    n2, m2 = children[i + 2]\n",
    "                    if isinstance(m2, (nn.ReLU, nn.ReLU6)):\n",
    "                        patterns.append([n0, n1, n2])\n",
    "                        i += 3\n",
    "                        continue\n",
    "                patterns.append([n0, n1])\n",
    "                i += 2\n",
    "                continue\n",
    "        i += 1\n",
    "    return patterns\n",
    "\n",
    "def fuse_bn_recursively(model: nn.Module) -> nn.Module:\n",
    "    assert not model.training, \"Call model.eval() before BN folding\"\n",
    "    for _, child in model.named_children():\n",
    "        fuse_bn_recursively(child)\n",
    "    patterns = _find_conv_bn_relu_sequences(model)\n",
    "    if patterns:\n",
    "        torch.quantization.fuse_modules(model, patterns, inplace=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7410828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ptq(model: nn.Module, calibration_loader, mode: Literal[\"per_tensor\", \"per_channel\"] = \"per_channel\",\n",
    "            fold_bn: bool = False, num_calibration_batches: int = 10, custom_fusion_patterns: Optional[List[List[str]]] = None) -> nn.Module:\n",
    "    \n",
    "    backend = \"fbgemm\"\n",
    "    torch.backends.quantized.engine = backend\n",
    "\n",
    "    model = copy.deepcopy(model).cpu().eval()\n",
    "\n",
    "    if fold_bn:\n",
    "        if custom_fusion_patterns is not None:\n",
    "            torch.quantization.fuse_modules(\n",
    "                model, custom_fusion_patterns, inplace=True\n",
    "            )\n",
    "        else:\n",
    "            fuse_bn_recursively(model)\n",
    "\n",
    "    if mode == \"per_channel\":\n",
    "        qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "    else:\n",
    "        qconfig = torch.quantization.QConfig(\n",
    "            activation=torch.quantization.MinMaxObserver.with_args(\n",
    "                dtype=torch.quint8, qscheme=torch.per_tensor_affine\n",
    "            ),\n",
    "            weight=torch.quantization.MinMaxObserver.with_args(\n",
    "                dtype=torch.qint8, qscheme=torch.per_tensor_symmetric\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    model.qconfig = qconfig\n",
    "    model_prepared = torch.quantization.prepare(model, inplace=False)\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(calibration_loader):\n",
    "            if i >= num_calibration_batches:\n",
    "                break\n",
    "            model_prepared(data.cpu())\n",
    "\n",
    "    model_quantized = torch.quantization.convert(model_prepared, inplace=False)\n",
    "    return model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f80c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                              Acc     Latency      Size\n",
      "============================================================\n",
      "CNN FP32 without layer fusion   76.63%      8.63ms   0.678MB\n",
      "CNN FP32 with layer fusion      76.63%      7.86ms   0.671MB\n"
     ]
    }
   ],
   "source": [
    "cnn_fused = copy.deepcopy(cnn_fp32).cpu().eval()\n",
    "torch.quantization.fuse_modules(cnn_fused, CNN_OBS.CNN_FUSION_PATTERNS, inplace=True)\n",
    "\n",
    "lat_fused  = measure_inference_time(cnn_fused, val_loader, device)\n",
    "acc_fused  = evaluate_model(cnn_fused, val_loader, device)\n",
    "size_fused = get_model_size(cnn_fused)\n",
    "\n",
    "print(f\"{'Model':<30} {'Acc':>7}  {'Latency':>10}  {'Size':>8}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'CNN FP32 without layer fusion':<30} {acc_cnn_fp32:>6.2f}%  {time_cnn_fp32:>8.2f}ms  {size_cnn_fp32:>6.3f}MB\")\n",
    "print(f\"{'CNN FP32 with layer fusion':<30} {acc_fused:>6.2f}%  {lat_fused:>8.2f}ms  {size_fused:>6.3f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "\n",
    "cnn_quant_base = CNNOBS(num_classes=10)\n",
    "cnn_quant_base.load_state_dict(torch.load(\"best_cnn_fp32.pth\", map_location=\"cpu\", weights_only=True))\n",
    "\n",
    "print(\"Quantizando: per_tensor ...\")\n",
    "cnn_pt = apply_ptq(cnn_quant_base, val_loader, mode=\"per_tensor\",  fold_bn=False, custom_fusion_patterns=CNN.CNN_FUSION_PATTERNS)\n",
    "\n",
    "print(\"Quantizando: per_tensor + fold_bn ...\")\n",
    "cnn_pt_fold = apply_ptq(cnn_quant_base, val_loader, mode=\"per_tensor\",  fold_bn=True, custom_fusion_patterns=CNN.CNN_FUSION_PATTERNS)\n",
    "\n",
    "print(\"Quantizando: per_channel ...\")\n",
    "cnn_pc = apply_ptq(cnn_quant_base, val_loader, mode=\"per_channel\", fold_bn=False, custom_fusion_patterns=CNN.CNN_FUSION_PATTERNS)\n",
    "\n",
    "print(\"Quantizando: per_channel + fold_bn ...\")\n",
    "cnn_pc_fold = apply_ptq(cnn_quant_base, val_loader, mode=\"per_channel\", fold_bn=True, custom_fusion_patterns=CNN.CNN_FUSION_PATTERNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all(model, name):\n",
    "    acc  = evaluate_model(model, val_loader, device)\n",
    "    lat  = measure_inference_time(model, val_loader, device)\n",
    "    size = get_model_size(model)\n",
    "    print(f\"{name:<35} acc={acc:.2f}%  lat={lat:.2f}ms  size={size:.3f}MB\")\n",
    "    return acc, lat, size\n",
    "\n",
    "print(f\"{'Modelo':<35} {'Acc':>8}  {'Latência':>10}  {'Tamanho':>9}\")\n",
    "print(\"-\" * 70)\n",
    "acc_fp32_cnn,  lat_fp32_cnn,  size_fp32_cnn  = eval_all(cnn_fp32,      \"CNN FP32\")\n",
    "acc_pt,        lat_pt,        size_pt         = eval_all(cnn_pt,       \"CNN per_tensor\")\n",
    "acc_pt_fold,   lat_pt_fold,   size_pt_fold    = eval_all(cnn_pt_fold,  \"CNN per_tensor + fold_bn\")\n",
    "acc_pc,        lat_pc,        size_pc         = eval_all(cnn_pc,       \"CNN per_channel\")\n",
    "acc_pc_fold,   lat_pc_fold,   size_pc_fold    = eval_all(cnn_pc_fold,  \"CNN per_channel + fold_bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std =[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Recria o val loader com transforms de ImageNet para avaliação dos modelos pré-treinados\n",
    "import torchvision.datasets as dsets\n",
    "cifar10_imagenet_val = dsets.CIFAR10(\n",
    "    \"../data\", train=False, download=True, transform=imagenet_transform\n",
    ")\n",
    "imagenet_val_loader = torch.utils.data.DataLoader(\n",
    "    cifar10_imagenet_val, batch_size=64, shuffle=False, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "def evaluate_pretrained_variants(model_name: str):\n",
    "    \"\"\"\n",
    "    Carrega, quantiza e avalia um modelo pré-treinado nas 4 variantes PTQ.\n",
    "    Retorna dict com acc, lat, size para fp32 e cada variante.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Modelo: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    torch.backends.quantized.engine = \"fbgemm\"\n",
    "\n",
    "    # FP32 baseline\n",
    "    model_fp32 = prepare_pretrained_model(model_name, num_classes=1000, wrap=False)\n",
    "    model_fp32.eval().cpu()\n",
    "\n",
    "    acc_fp32_ = evaluate_model(model_fp32, imagenet_val_loader, device)\n",
    "    lat_fp32_ = measure_inference_time(model_fp32, imagenet_val_loader, device, num_batches=10)\n",
    "    sz_fp32_  = get_model_size(model_fp32)\n",
    "    print(f\"  FP32       acc={acc_fp32_:.2f}%  lat={lat_fp32_:.1f}ms  size={sz_fp32_:.1f}MB\")\n",
    "\n",
    "    # Para quantização precisamos do wrapper\n",
    "    base_wrapped = prepare_pretrained_model(model_name, num_classes=1000, wrap=True)\n",
    "\n",
    "    variants = {\n",
    "        \"per_tensor\":         dict(mode=\"per_tensor\",  fold_bn=False),\n",
    "        \"per_tensor_foldbn\":  dict(mode=\"per_tensor\",  fold_bn=True),\n",
    "        \"per_channel\":        dict(mode=\"per_channel\", fold_bn=False),\n",
    "        \"per_channel_foldbn\": dict(mode=\"per_channel\", fold_bn=True),\n",
    "    }\n",
    "\n",
    "    row = {\n",
    "        \"model\": model_name,\n",
    "        \"fp32_acc\": acc_fp32_, \"fp32_lat\": lat_fp32_, \"fp32_size\": sz_fp32_,\n",
    "    }\n",
    "\n",
    "    for tag, kwargs in variants.items():\n",
    "        q_model = apply_ptq(base_wrapped, imagenet_val_loader,\n",
    "                            num_calibration_batches=10, **kwargs)\n",
    "        acc_ = evaluate_model(q_model, imagenet_val_loader, device)\n",
    "        lat_ = measure_inference_time(q_model, imagenet_val_loader, device, num_batches=10)\n",
    "        sz_  = get_model_size(q_model)\n",
    "        print(f\"  {tag:<22} acc={acc_:.2f}%  lat={lat_:.1f}ms  size={sz_:.1f}MB\")\n",
    "        row[f\"{tag}_acc\"]  = acc_\n",
    "        row[f\"{tag}_lat\"]  = lat_\n",
    "        row[f\"{tag}_size\"] = sz_\n",
    "\n",
    "    return row\n",
    "\n",
    "# ── Executar para todos os modelos pré-treinados ──────────────────────────────\n",
    "pretrained_names = [\"mobilenet_v2\", \"resnet50\", \"resnext50_32x4d\", \"efficientnet_b0\"]\n",
    "pretrained_results = []\n",
    "\n",
    "for name in pretrained_names:\n",
    "    row = evaluate_pretrained_variants(name)\n",
    "    pretrained_results.append(row)\n",
    "\n",
    "print(\"\\nQuantização de modelos pré-treinados concluída!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# ── Tabela 1: Modelo | Variante | Latência (ms) | Tamanho (MB) ────────────────\n",
    "rows_t1 = []\n",
    "\n",
    "# CNN (custom)\n",
    "cnn_variants_t1 = [\n",
    "    (\"CNN\",  \"FP32\",                   lat_fp32_cnn,  size_fp32_cnn),\n",
    "    (\"CNN\",  \"per_tensor\",             lat_pt,        size_pt),\n",
    "    (\"CNN\",  \"per_tensor + fold_bn\",   lat_pt_fold,   size_pt_fold),\n",
    "    (\"CNN\",  \"per_channel\",            lat_pc,        size_pc),\n",
    "    (\"CNN\",  \"per_channel + fold_bn\",  lat_pc_fold,   size_pc_fold),\n",
    "]\n",
    "rows_t1.extend(cnn_variants_t1)\n",
    "\n",
    "# Modelos pré-treinados\n",
    "for r in pretrained_results:\n",
    "    m = r[\"model\"]\n",
    "    rows_t1.append((m, \"FP32\",                   r[\"fp32_lat\"],                r[\"fp32_size\"]))\n",
    "    rows_t1.append((m, \"per_tensor\",             r[\"per_tensor_lat\"],          r[\"per_tensor_size\"]))\n",
    "    rows_t1.append((m, \"per_tensor + fold_bn\",   r[\"per_tensor_foldbn_lat\"],   r[\"per_tensor_foldbn_size\"]))\n",
    "    rows_t1.append((m, \"per_channel\",            r[\"per_channel_lat\"],         r[\"per_channel_size\"]))\n",
    "    rows_t1.append((m, \"per_channel + fold_bn\",  r[\"per_channel_foldbn_lat\"],  r[\"per_channel_foldbn_size\"]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"TABELA 1 — Latência e Tamanho dos Modelos\")\n",
    "print(\"=\"*65)\n",
    "print(tabulate(\n",
    "    [[m, v, f\"{lat:.2f}\", f\"{sz:.3f}\"] for m, v, lat, sz in rows_t1],\n",
    "    headers=[\"Modelo\", \"Variante\", \"Latência (ms/batch)\", \"Tamanho (MB)\"],\n",
    "    tablefmt=\"fancy_grid\"\n",
    "))\n",
    "\n",
    "# ── Tabela 2: Modelo | FP32 | per_tensor | per_tensor_fold | per_channel | per_channel_fold ──\n",
    "rows_t2 = []\n",
    "\n",
    "# CNN\n",
    "rows_t2.append([\n",
    "    \"CNN\",\n",
    "    f\"{acc_fp32_cnn:.2f}\",\n",
    "    f\"{acc_pt:.2f}\",\n",
    "    f\"{acc_pt_fold:.2f}\",\n",
    "    f\"{acc_pc:.2f}\",\n",
    "    f\"{acc_pc_fold:.2f}\",\n",
    "])\n",
    "\n",
    "# Pré-treinados\n",
    "for r in pretrained_results:\n",
    "    rows_t2.append([\n",
    "        r[\"model\"],\n",
    "        f\"{r['fp32_acc']:.2f}\",\n",
    "        f\"{r['per_tensor_acc']:.2f}\",\n",
    "        f\"{r['per_tensor_foldbn_acc']:.2f}\",\n",
    "        f\"{r['per_channel_acc']:.2f}\",\n",
    "        f\"{r['per_channel_foldbn_acc']:.2f}\",\n",
    "    ])\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"TABELA 2 — Acurácia (%) por Estratégia de Quantização\")\n",
    "print(\"=\"*90)\n",
    "print(tabulate(\n",
    "    rows_t2,\n",
    "    headers=[\n",
    "        \"Modelo\", \"FP32 (%)\", \"per_tensor (%)\",\n",
    "        \"per_tensor\\n+fold_bn (%)\", \"per_channel (%)\",\n",
    "        \"per_channel\\n+fold_bn (%)\"\n",
    "    ],\n",
    "    tablefmt=\"fancy_grid\"\n",
    "))\n",
    "\n",
    "print(\"\\n[Legenda]\")\n",
    "print(\"  fold_bn   = BatchNorm absorvido nos pesos Conv antes da quantização\")\n",
    "print(\"  per_tensor = uma escala por tensor inteiro (pesos + ativações)\")\n",
    "print(\"  per_channel= uma escala por canal de saída (pesos) — padrão fbgemm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee97bb2",
   "metadata": {},
   "source": [
    "#### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizableWrapper(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.quant   = QuantStub()\n",
    "        self.model   = model\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "def prepare_pretrained_model(model_name: str, num_classes: int = 1000, wrap: bool = True) -> nn.Module:\n",
    "    weights_map = {\n",
    "        \"mobilenet_v2\":      (models.mobilenet_v2,       models.MobileNet_V2_Weights.IMAGENET1K_V2),\n",
    "        \"mobilenet_v3_small\":(models.mobilenet_v3_small, models.MobileNet_V3_Small_Weights.IMAGENET1K_V1),\n",
    "        \"efficientnet_b0\":   (models.efficientnet_b0,    models.EfficientNet_B0_Weights.IMAGENET1K_V1),\n",
    "        \"resnet50\":          (models.resnet50,           models.ResNet50_Weights.IMAGENET1K_V2),\n",
    "        \"resnext50_32x4d\":   (models.resnext50_32x4d,   models.ResNeXt50_32X4D_Weights.IMAGENET1K_V2),\n",
    "    }\n",
    "\n",
    "    if model_name not in weights_map:\n",
    "        raise ValueError(f\"Model '{model_name}' not supported. Choose from: {list(weights_map)}\")\n",
    "\n",
    "    constructor, weights = weights_map[model_name]\n",
    "    model = constructor(weights=weights)\n",
    "\n",
    "    if num_classes != 1000:\n",
    "        if model_name == \"mobilenet_v2\":\n",
    "            model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "        elif model_name == \"mobilenet_v3_small\":\n",
    "            model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "        elif model_name == \"efficientnet_b0\":\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        elif model_name in (\"resnet50\", \"resnext50_32x4d\"):\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    return QuantizableWrapper(model) if wrap else model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fcbef",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.ao.quantization as quantization\n",
    "\n",
    "# Configuração para INT4\n",
    "# Definimos o range de 0 a 15 (4 bits)\n",
    "qconfig_int4 = quantization.FakeQuantize.with_args(\n",
    "    observer=quantization.MinMaxObserver,\n",
    "    quant_min=0,\n",
    "    quant_max=15,\n",
    "    dtype=torch.quint8,\n",
    "    qscheme=torch.per_tensor_affine\n",
    ")\n",
    "\n",
    "# Para INT2 (0 a 3)\n",
    "qconfig_int2 = quantization.FakeQuantize.with_args(\n",
    "    observer=quantization.MinMaxObserver,\n",
    "    quant_min=0,\n",
    "    quant_max=3,\n",
    "    dtype=torch.quint8,\n",
    "    qscheme=torch.per_tensor_affine\n",
    ")\n",
    "\n",
    "class BinaryActivation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.sign(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-Through Estimator (STE)\n",
    "        return grad_output\n",
    "\n",
    "class BinaryLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Binariza pesos e entradas durante o forward\n",
    "        bw = torch.sign(self.weight)\n",
    "        bx = torch.sign(x)\n",
    "        return F.linear(bx, bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96b2f3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=512, bias=True\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(\n",
       "    in_features=512, out_features=10, bias=True\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int4 = QuantizedMLP(input_dim=784, hidden_dim=512, output_dim=10)\n",
    "model_int4.qconfig = quantization.QConfig(activation=qconfig_int4, weight=qconfig_int4)\n",
    "model_prepared = quantization.prepare_qat(model_int4)\n",
    "model_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c959e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prepared.eval()\n",
    "with torch.no_grad():\n",
    "    for images, _ in calibration_loader:\n",
    "        model_prepared(images)\n",
    "\n",
    "model_int4 = quantization.convert(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cf9e971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 Accuracy: 13.65%\n",
      "FP32 Latency:  3.09 ms/batch\n",
      "FP32 Size:  1.56528 MB\n"
     ]
    }
   ],
   "source": [
    "acc_model_prepared = validate_model(model_prepared, val_loader, device, desc='Val Quantized')\n",
    "lat_model_prepared = measure_inference_time(model_prepared, val_loader, device)\n",
    "model_size_model_prepared = get_model_size(model_prepared)\n",
    "\n",
    "print(f\"FP32 Accuracy: {acc_model_prepared:.2f}%\")\n",
    "print(f\"FP32 Latency:  {lat_model_prepared:.2f} ms/batch\")\n",
    "print(f\"FP32 Size:  {model_size_model_prepared:.5f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
