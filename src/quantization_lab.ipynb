{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c1efbe",
   "metadata": {},
   "source": [
    "1. PTQ Static with calibration MinMax (per-tensor and per-channel via fbgemm)\n",
    "2. CNN, CNN with Early Exit and Pre-trained models (MobileNetV2, ResNet50, ResNeXt50, EfficientNet-B0)\n",
    "3. Dataset: CIFAR-10, CIFAR-100, Tiny ImageNet\n",
    "4. Target: CPU inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fee44c",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a97524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Literal, Dict, Any, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.ao.quantization\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.quantization import QuantStub, DeQuantStub, prepare_qat, convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a24490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cpu\n",
      "['none', 'onednn', 'x86', 'fbgemm']\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(torch.backends.quantized.supported_engines)\n",
    "device  = torch.device(\"cpu\")\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795056a",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d85b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetName = Literal[\"cifar10\", \"cifar100\", \"tiny_imagenet\"]\n",
    "\n",
    "class DataloaderManager:\n",
    "    CONFIGS = {\n",
    "        \"cifar10\": {\n",
    "            \"size\": 32, \"padding\": 4, \"num_classes\": 10,\n",
    "            \"stats\": ((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "        },\n",
    "        \"cifar100\": {\n",
    "            \"size\": 32, \"padding\": 4, \"num_classes\": 100,\n",
    "            \"stats\": ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "        },\n",
    "        \"tiny_imagenet\": {\n",
    "            \"size\": 64, \"padding\": 8, \"num_classes\": 200,\n",
    "            \"stats\": ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, root_dir: str, dataset: DatasetName):\n",
    "        self.root = Path(root_dir)\n",
    "        self.dataset = dataset\n",
    "        self.cfg = self.CONFIGS[dataset]\n",
    "        \n",
    "    def _get_transforms(self, train: bool):\n",
    "        mean, std = self.cfg[\"stats\"]\n",
    "        tf_list = []\n",
    "        if train:\n",
    "            tf_list.extend([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop(self.cfg[\"size\"], padding=self.cfg[\"padding\"])\n",
    "            ])\n",
    "        tf_list.extend([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "        return transforms.Compose(tf_list)\n",
    "\n",
    "    def _get_dataset_instance(self, train: bool):\n",
    "        tf = self._get_transforms(train)\n",
    "        if self.dataset.startswith(\"cifar\"):\n",
    "            ds_cls = datasets.CIFAR10 if self.dataset == \"cifar10\" else datasets.CIFAR100\n",
    "            return ds_cls(self.root, train=train, download=True, transform=tf)\n",
    "        \n",
    "        path = self.root / \"tiny-imagenet-200\" / (\"train\" if train else \"val\")\n",
    "        return datasets.ImageFolder(str(path), transform=tf)\n",
    "\n",
    "    def get_loaders(self, batch_size: int, num_workers: int):\n",
    "        train_ds = self._get_dataset_instance(train=True)\n",
    "        val_ds   = self._get_dataset_instance(train=False)\n",
    "\n",
    "        loader_args = {\"batch_size\": batch_size, \"num_workers\": num_workers, \"pin_memory\": True}\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, **loader_args)\n",
    "        val_loader   = torch.utils.data.DataLoader(val_ds, shuffle=False, **loader_args)\n",
    "\n",
    "        self._print_summary(train_loader, val_loader, batch_size)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def _print_summary(self, train_loader, val_loader, batch_size):\n",
    "        print(f\"Dataset      : {self.dataset}\")\n",
    "        print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "        print(f\"Val samples  : {len(val_loader.dataset)}\")\n",
    "        print(f\"Classes      : {self.cfg['num_classes']}\")\n",
    "        print(f\"Batch size   : {batch_size}\")\n",
    "        print(f\"Train batches: {len(train_loader)}\")\n",
    "        print(f\"Val batches  : {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3faaf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset      : cifar10\n",
      "Train samples: 50000\n",
      "Val samples  : 10000\n",
      "Classes      : 10\n",
      "Batch size   : 64\n",
      "Train batches: 782\n",
      "Val batches  : 157\n"
     ]
    }
   ],
   "source": [
    "manager = DataloaderManager(root_dir=\"../data\", dataset=\"cifar10\")\n",
    "train_loader, val_loader = manager.get_loaders(batch_size=64, num_workers=4)\n",
    "num_classes = manager.cfg[\"num_classes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a4ddb",
   "metadata": {},
   "source": [
    "### Training and evaluation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61061ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    device, \n",
    "    epochs: int, \n",
    "    lr: float, \n",
    "    model_name: str = \"model\"\n",
    ") -> nn.Module:\n",
    "    \n",
    "    print(f\"Training {model_name} | Epochs: {epochs} | LR: {lr}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{running_loss/len(train_loader):.4f}\")\n",
    "            \n",
    "        patience = 5\n",
    "        trigger_times = 0\n",
    "        acc = validate_model(model, val_loader, device, desc=f\"Eval Ep {epoch+1}\")\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Result Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f} | Acc = {acc:.2f}%\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), f\"best_{model_name}.pth\")\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"best_{model_name}.pth\", weights_only=True))\n",
    "    return model\n",
    "\n",
    "def validate_model(model: nn.Module, data_loader, device, desc=\"Validating\") -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(data_loader, desc=desc, leave=False):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "    return 100.0 * correct / total\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader, device) -> float:\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            pred    = model(data).argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total   += target.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "def measure_inference_time(model: nn.Module, test_loader, device, num_batches: int = 20) -> float:\n",
    "    \"\"\"Return mean latency per batch in milliseconds.\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            if i < 3:   # warmup\n",
    "                _ = model(data)\n",
    "                continue\n",
    "            t0 = time.time()\n",
    "            _  = model(data)\n",
    "            times.append(time.time() - t0)\n",
    "    return sum(times) / len(times) * 1000\n",
    "\n",
    "def get_model_size(model: nn.Module) -> float:\n",
    "    buf = io.BytesIO()\n",
    "    torch.save(model.state_dict(), buf)\n",
    "    return buf.tell() / 1024 / 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1e9fd",
   "metadata": {},
   "source": [
    "# PTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd12e2",
   "metadata": {},
   "source": [
    "## Calibration MinMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b52e9",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92049c3",
   "metadata": {},
   "source": [
    "#### CNN without observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3e7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NO_OBS(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(64 * 8 * 8, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acbe3c4",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffcf010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training experiment1_cnn_fp32 | Epochs: 5 | LR: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 782/782 [00:23<00:00, 32.61it/s, loss=1.5351]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 1: Loss = 1.5351 | Acc = 59.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 782/782 [00:23<00:00, 33.77it/s, loss=1.1884]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 2: Loss = 1.1884 | Acc = 63.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 782/782 [00:23<00:00, 33.85it/s, loss=1.0591]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 3: Loss = 1.0591 | Acc = 67.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 782/782 [00:23<00:00, 33.83it/s, loss=0.9651]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 4: Loss = 0.9651 | Acc = 70.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 782/782 [00:23<00:00, 33.20it/s, loss=0.9092]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 5: Loss = 0.9092 | Acc = 71.80%\n",
      "Results:\n",
      "Acc     : 71.80%\n",
      "Latency : 5.66 ms/batch\n",
      "Size    : 0.23249 MB\n"
     ]
    }
   ],
   "source": [
    "cnn_fp32 = CNN_NO_OBS(num_classes=10).to(device)\n",
    "cnn_fp32 = train_model(\n",
    "    cnn_fp32, train_loader, val_loader, device,\n",
    "    epochs=5, lr=0.01, model_name=\"experiment1_cnn_fp32\"\n",
    ")\n",
    "\n",
    "acc_cnn_fp32  = evaluate_model(cnn_fp32, val_loader, device)\n",
    "time_cnn_fp32 = measure_inference_time(cnn_fp32, val_loader, device)\n",
    "size_cnn_fp32 = get_model_size(cnn_fp32)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"Acc     : {acc_cnn_fp32:.2f}%\")\n",
    "print(f\"Latency : {time_cnn_fp32:.2f} ms/batch\")\n",
    "print(f\"Size    : {size_cnn_fp32:.5f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0590479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Structure:\n",
      "conv.0.weight -> torch.Size([32, 3, 3, 3])\n",
      "conv.0.bias -> torch.Size([32])\n",
      "conv.3.weight -> torch.Size([64, 32, 3, 3])\n",
      "conv.3.bias -> torch.Size([64])\n",
      "fc.weight -> torch.Size([10, 4096])\n",
      "fc.bias -> torch.Size([10])\n",
      "==========================================\n",
      "Weights Statistics:\n",
      "\n",
      "conv.0.weight:\n",
      "tensor([-0.0083, -0.2532,  0.1813, -0.3246,  0.0511, -0.0176,  0.1043,  0.1120,\n",
      "         0.0295,  0.1621])\n",
      "Dtype: torch.float32\n",
      "Shape: (32, 3, 3, 3)\n",
      "Mean: 0.000135\n",
      "Std:  0.197591\n",
      "------------------------------------------\n",
      "conv.0.bias:\n",
      "tensor([-0.0279, -0.4891, -0.1486, -0.4258, -0.4363, -0.2449, -0.4396, -0.1404,\n",
      "        -0.0152, -0.1232])\n",
      "Dtype: torch.float32\n",
      "Shape: (32,)\n",
      "Mean: -0.236521\n",
      "Std:  0.191881\n",
      "------------------------------------------\n",
      "conv.3.weight:\n",
      "tensor([-0.0315, -0.0453,  0.0132, -0.0238, -0.0111,  0.0778,  0.0407,  0.0863,\n",
      "         0.1192,  0.0064])\n",
      "Dtype: torch.float32\n",
      "Shape: (64, 32, 3, 3)\n",
      "Mean: -0.012429\n",
      "Std:  0.056563\n",
      "------------------------------------------\n",
      "conv.3.bias:\n",
      "tensor([ 0.0652,  0.0295, -0.1405,  0.0456,  0.1061,  0.0802,  0.1253,  0.0642,\n",
      "         0.3279,  0.1011])\n",
      "Dtype: torch.float32\n",
      "Shape: (64,)\n",
      "Mean: 0.067707\n",
      "Std:  0.162950\n",
      "------------------------------------------\n",
      "fc.weight:\n",
      "tensor([-0.0076, -0.0259, -0.0107, -0.0031, -0.0133, -0.0326, -0.0204, -0.0246,\n",
      "        -0.0043,  0.0120])\n",
      "Dtype: torch.float32\n",
      "Shape: (10, 4096)\n",
      "Mean: 0.000059\n",
      "Std:  0.037281\n",
      "------------------------------------------\n",
      "fc.bias:\n",
      "tensor([-0.1754, -0.2433, -0.0503,  0.0955,  0.3588, -0.0609,  0.0546, -0.1842,\n",
      "         0.2560, -0.0307])\n",
      "Dtype: torch.float32\n",
      "Shape: (10,)\n",
      "Mean: 0.002018\n",
      "Std:  0.193614\n",
      "------------------------------------------\n",
      "Datatype model:\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"best_experiment1_cnn_fp32.pth\", map_location=\"cpu\", weights_only=True)\n",
    "cnn_fp32.load_state_dict(state_dict)\n",
    "cnn_fp32.eval()\n",
    "\n",
    "print(\"Weights Structure:\")\n",
    "for name, param in cnn_fp32.named_parameters():\n",
    "    print(f\"{name} -> {param.shape}\")\n",
    "    \n",
    "print('='*42)\n",
    "\n",
    "print(\"Weights Statistics:\\n\")\n",
    "\n",
    "for name, param in cnn_fp32.named_parameters():\n",
    "    print(f\"{name}:\")\n",
    "    print(param.data.view(-1)[:10])\n",
    "    print(f\"Dtype: {param.dtype}\")\n",
    "    print(f\"Shape: {tuple(param.shape)}\")\n",
    "    print(f\"Mean: {param.data.mean().item():.6f}\")\n",
    "    print(f\"Std:  {param.data.std().item():.6f}\")\n",
    "    print('-'*42)\n",
    "  \n",
    "print('Datatype model:')  \n",
    "print(next(cnn_fp32.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e6d58",
   "metadata": {},
   "source": [
    "#### CNN with observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afd7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_OBS(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.quant   = QuantStub()\n",
    "        self.dequant = DeQuantStub() \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 8 * 8, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad341859",
   "metadata": {},
   "source": [
    "#### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f4ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_OBS(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(\n",
       "      3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(\n",
       "      32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(\n",
       "    in_features=4096, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_quantized = CNN_OBS(num_classes=10).to(device)\n",
    "cnn_quantized.load_state_dict(cnn_fp32.state_dict())\n",
    "cnn_quantized.eval()\n",
    "\n",
    "cnn_quantized.qconfig = torch.ao.quantization.default_qconfig\n",
    "cnn_quantized = torch.ao.quantization.prepare(cnn_quantized)\n",
    "cnn_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de3b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 Accuracy: 71.80%\n",
      "FP32 Latency:  6.13 ms/batch\n",
      "FP32 Size:  0.24 MB\n"
     ]
    }
   ],
   "source": [
    "acc_quantized = validate_model(cnn_quantized, val_loader, device, desc='Val Quantized')\n",
    "lat_quantized = measure_inference_time(cnn_quantized, val_loader, device)\n",
    "model_size_quantized = get_model_size(cnn_quantized)\n",
    "\n",
    "print(f\"FP32 Accuracy: {acc_quantized:.2f}%\")\n",
    "print(f\"FP32 Latency:  {lat_quantized:.2f} ms/batch\")\n",
    "print(f\"FP32 Size:  {model_size_quantized:.5f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9bf03",
   "metadata": {},
   "source": [
    "#### Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f930785d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_OBS(\n",
       "  (quant): Quantize(scale=tensor([0.0324]), zero_point=tensor([61]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (conv): Sequential(\n",
       "    (0): QuantizedConv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.18029893934726715, zero_point=66, padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.20834548771381378, zero_point=87, padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): QuantizedLinear(in_features=4096, out_features=10, scale=0.25371870398521423, zero_point=57, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_quantized\n",
    "cnn_quantized = torch.ao.quantization.convert(cnn_quantized)\n",
    "cnn_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f3d3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Statistics:\n",
      "\n",
      "tensor([[[[  -1,  -43,   31],\n",
      "          [ -55,    9,   -3],\n",
      "          [  18,   19,    5]],\n",
      "\n",
      "         [[  28,   12,   25],\n",
      "          [ -13,   26,   43],\n",
      "          [ -13,   31,   19]],\n",
      "\n",
      "         [[ -29,  -19,    8],\n",
      "          [ -20,  -50,  -25],\n",
      "          [ -38,    7,   12]]],\n",
      "\n",
      "\n",
      "        [[[  49,   43,   15],\n",
      "          [  -1,  -31,  -15],\n",
      "          [   0,  -48,  -33]],\n",
      "\n",
      "         [[  32,   12,  -14],\n",
      "          [   3,  -39,  -26],\n",
      "          [  25,   15,    0]],\n",
      "\n",
      "         [[  51,  -14,   31],\n",
      "          [  20,  -32,    8],\n",
      "          [ -20,  -12,    6]]],\n",
      "\n",
      "\n",
      "        [[[  -7,  -20,  -13],\n",
      "          [  -9,  -21,  -30],\n",
      "          [ -48,  -36,  -29]],\n",
      "\n",
      "         [[ -27,    6,   -3],\n",
      "          [   3,  -23,    4],\n",
      "          [ -28,  -18,   19]],\n",
      "\n",
      "         [[  20,   35,   28],\n",
      "          [  37,   67,   32],\n",
      "          [  34,   44,   37]]],\n",
      "\n",
      "\n",
      "        [[[ -66,  -25,  -60],\n",
      "          [ -31,   21,  -22],\n",
      "          [  47,   46,   34]],\n",
      "\n",
      "         [[ -33,   16,  -13],\n",
      "          [ -33,   34,    9],\n",
      "          [  14,   41,  -12]],\n",
      "\n",
      "         [[  -4,   -3,    4],\n",
      "          [ -22,   16,   -7],\n",
      "          [  29,   33,    6]]],\n",
      "\n",
      "\n",
      "        [[[  16,   51,   57],\n",
      "          [  25,   45,   51],\n",
      "          [ -32,  -12,   41]],\n",
      "\n",
      "         [[ -22,  -28,  -34],\n",
      "          [ -63,  -52,  -21],\n",
      "          [ -54,   -7,  -37]],\n",
      "\n",
      "         [[  29,    3,  -10],\n",
      "          [  -4,  -21,  -15],\n",
      "          [   5,    4,  -12]]],\n",
      "\n",
      "\n",
      "        [[[  40,   47,   47],\n",
      "          [  24,    5,   14],\n",
      "          [   9,    9,    4]],\n",
      "\n",
      "         [[ -44,   -9,   18],\n",
      "          [  17,  -18,  -32],\n",
      "          [ -27,   -1,  -40]],\n",
      "\n",
      "         [[ -43,   25,   41],\n",
      "          [ -27,  -27,   24],\n",
      "          [  19,   28,    1]]],\n",
      "\n",
      "\n",
      "        [[[ -60,   13,   -5],\n",
      "          [   8,   34,    5],\n",
      "          [ -16,   32,  -19]],\n",
      "\n",
      "         [[   3,   37,  -17],\n",
      "          [  -5,   15,   25],\n",
      "          [ -11,   33,   30]],\n",
      "\n",
      "         [[ -22,   41,  -40],\n",
      "          [  19,   37,   19],\n",
      "          [ -25,  -40,  -54]]],\n",
      "\n",
      "\n",
      "        [[[ -10,  -50,    9],\n",
      "          [  13,  -30,  -35],\n",
      "          [ -24,  -31,    9]],\n",
      "\n",
      "         [[  34,   34,   77],\n",
      "          [  32,   32,   43],\n",
      "          [  53,   -5,   29]],\n",
      "\n",
      "         [[ -37,  -26,  -22],\n",
      "          [  17,  -52,  -49],\n",
      "          [ -42,    2,  -10]]],\n",
      "\n",
      "\n",
      "        [[[ -38,   54,  -13],\n",
      "          [  58,  -21,  -34],\n",
      "          [  20,  -58,   28]],\n",
      "\n",
      "         [[ -34,   43,   -1],\n",
      "          [  54,   -2,  -52],\n",
      "          [   2,  -47,   32]],\n",
      "\n",
      "         [[ -33,   37,  -17],\n",
      "          [  23,  -24,    9],\n",
      "          [   9,  -22,   29]]],\n",
      "\n",
      "\n",
      "        [[[ -54,    4,   48],\n",
      "          [ -52,   38,   17],\n",
      "          [ -20,   22,    9]],\n",
      "\n",
      "         [[ -69,   16,   14],\n",
      "          [ -58,   34,   57],\n",
      "          [ -43,    7,   22]],\n",
      "\n",
      "         [[ -64,    4,   43],\n",
      "          [ -72,   31,   29],\n",
      "          [ -15,   10,   38]]],\n",
      "\n",
      "\n",
      "        [[[ -43,  -52,  -15],\n",
      "          [   9,  -21,   41],\n",
      "          [  53,   62,   33]],\n",
      "\n",
      "         [[ -35,  -14,    3],\n",
      "          [ -10,   -5,   -2],\n",
      "          [  14,    5,   11]],\n",
      "\n",
      "         [[  12,  -12,   -7],\n",
      "          [ -12,  -26,   27],\n",
      "          [   8,    2,    5]]],\n",
      "\n",
      "\n",
      "        [[[   0,   -3,   19],\n",
      "          [ -23,   13,   17],\n",
      "          [  -3,  -30,   -7]],\n",
      "\n",
      "         [[ -27,  -13,   10],\n",
      "          [ -14,  -23,   36],\n",
      "          [ -40,    5,   23]],\n",
      "\n",
      "         [[ -15,    3,   46],\n",
      "          [  -3,   20,   44],\n",
      "          [ -11,   19,   52]]],\n",
      "\n",
      "\n",
      "        [[[ -20,   60,    6],\n",
      "          [   5,   62,   40],\n",
      "          [ -27,   17,   -2]],\n",
      "\n",
      "         [[  27,  -29,  -10],\n",
      "          [   8,  -42,  -40],\n",
      "          [ -23,  -21,   25]],\n",
      "\n",
      "         [[ -10,  -40,   -3],\n",
      "          [ -22,  -44,  -10],\n",
      "          [  55,    5,  -18]]],\n",
      "\n",
      "\n",
      "        [[[ -36,  -44,  -53],\n",
      "          [ -11,   -5,  -47],\n",
      "          [ -24,   21,  -40]],\n",
      "\n",
      "         [[  29,   24,    2],\n",
      "          [   7,   38,   36],\n",
      "          [  -5,   17,  -10]],\n",
      "\n",
      "         [[ -19,    6,  -26],\n",
      "          [ -12,   37,  -12],\n",
      "          [ -13,   18,    6]]],\n",
      "\n",
      "\n",
      "        [[[ -20,  -26,   20],\n",
      "          [  27,  -36,  -32],\n",
      "          [  27,  -13,   21]],\n",
      "\n",
      "         [[  11,  -21,   47],\n",
      "          [  24,  -46,  -19],\n",
      "          [  18,   34,  -17]],\n",
      "\n",
      "         [[ -21,  -22,   -1],\n",
      "          [  26,  -42,   10],\n",
      "          [  14,   36,   -3]]],\n",
      "\n",
      "\n",
      "        [[[  67,    4,   19],\n",
      "          [  25,  -21,  -45],\n",
      "          [ -10,  -10,  -27]],\n",
      "\n",
      "         [[ -18,    7,  -31],\n",
      "          [   6,    8,   -1],\n",
      "          [ -21,  -19,   16]],\n",
      "\n",
      "         [[ -64,  -44,   17],\n",
      "          [ -44,   36,   63],\n",
      "          [  35,   49,   23]]],\n",
      "\n",
      "\n",
      "        [[[  18,    0,  -44],\n",
      "          [ -16,   41,   40],\n",
      "          [ -37,   22,  -18]],\n",
      "\n",
      "         [[ -27,   45,    9],\n",
      "          [  -4,   60,   16],\n",
      "          [ -67,  -46,  -69]],\n",
      "\n",
      "         [[   0,   -4,   20],\n",
      "          [  35,   19,   18],\n",
      "          [  11,    5,  -38]]],\n",
      "\n",
      "\n",
      "        [[[ -52,   95,  -61],\n",
      "          [ -23,  103,  -74],\n",
      "          [ -59,   95,  -40]],\n",
      "\n",
      "         [[ -61,   89,  -55],\n",
      "          [ -80,  120,  -27],\n",
      "          [ -36,   91,  -47]],\n",
      "\n",
      "         [[ -21,   65,  -25],\n",
      "          [ -40,   89,  -45],\n",
      "          [ -41,   63,  -37]]],\n",
      "\n",
      "\n",
      "        [[[ -28,  -32,  -40],\n",
      "          [  24,   31,    2],\n",
      "          [  39,   37,   -3]],\n",
      "\n",
      "         [[  19,    8,   17],\n",
      "          [  11,  -26,   18],\n",
      "          [ -16,  -44,   16]],\n",
      "\n",
      "         [[  44,   42,   20],\n",
      "          [ -20,  -39,  -29],\n",
      "          [  -1,  -20,  -19]]],\n",
      "\n",
      "\n",
      "        [[[  24,   -4,    3],\n",
      "          [  31,   -7,  -37],\n",
      "          [  14,   36,  -29]],\n",
      "\n",
      "         [[  11,  -19,  -50],\n",
      "          [  65,  -13,  -69],\n",
      "          [  60,   17,  -54]],\n",
      "\n",
      "         [[  49,  -10,  -63],\n",
      "          [  58,   44,  -58],\n",
      "          [  26,   10,  -31]]],\n",
      "\n",
      "\n",
      "        [[[   8,  -30,  -30],\n",
      "          [ -69,   15,   30],\n",
      "          [  41,  -22,  -24]],\n",
      "\n",
      "         [[  33,   14,   17],\n",
      "          [ -70,   50,   54],\n",
      "          [  55,   14,  -58]],\n",
      "\n",
      "         [[ -11,  -20,   -1],\n",
      "          [ -48,  -10,   54],\n",
      "          [  54,    1,  -64]]],\n",
      "\n",
      "\n",
      "        [[[  -6,  -16,   34],\n",
      "          [ -29,   -8,  -35],\n",
      "          [ -28,  -18,    7]],\n",
      "\n",
      "         [[  39,   -7,  -37],\n",
      "          [  -6,  -11,  -53],\n",
      "          [  14,  -19,  -43]],\n",
      "\n",
      "         [[  24,   12,  -27],\n",
      "          [  12,   33,  -44],\n",
      "          [  57,   49,   10]]],\n",
      "\n",
      "\n",
      "        [[[  10,  -34,  -17],\n",
      "          [  47,  -12,    9],\n",
      "          [   5,  -11,   -8]],\n",
      "\n",
      "         [[  22,  -14,  -21],\n",
      "          [  42,   27,   16],\n",
      "          [  31,  -14,  -21]],\n",
      "\n",
      "         [[ -17,  -23,  -12],\n",
      "          [  16,  -10,  -20],\n",
      "          [   9,   14,  -29]]],\n",
      "\n",
      "\n",
      "        [[[ -18,  -14,  -42],\n",
      "          [  55,   57,   -4],\n",
      "          [ -32,  -10,   34]],\n",
      "\n",
      "         [[ -35,  -69,  -42],\n",
      "          [  36,   32,   21],\n",
      "          [  -1,   21,    6]],\n",
      "\n",
      "         [[ -48,  -38,   -6],\n",
      "          [  57,   53,   -5],\n",
      "          [  -4,  -29,   20]]],\n",
      "\n",
      "\n",
      "        [[[ -48,  -54,  113],\n",
      "          [  33,  -35,   10],\n",
      "          [  19,   85, -111]],\n",
      "\n",
      "         [[ -54,  -53,   95],\n",
      "          [   9,  -43,    7],\n",
      "          [  49,   73, -128]],\n",
      "\n",
      "         [[ -36,  -63,   98],\n",
      "          [  26,   -6,    5],\n",
      "          [  22,   77,  -90]]],\n",
      "\n",
      "\n",
      "        [[[ -12,   17,   32],\n",
      "          [  15,    4,   33],\n",
      "          [  24,   20,   29]],\n",
      "\n",
      "         [[ -25,   11,   -9],\n",
      "          [ -44,  -57,   11],\n",
      "          [ -12,  -57,  -38]],\n",
      "\n",
      "         [[   4,   18,   11],\n",
      "          [  -5,  -40,   17],\n",
      "          [  33,   -8,   10]]],\n",
      "\n",
      "\n",
      "        [[[  -9,   -5,  -20],\n",
      "          [  13,   14,   16],\n",
      "          [  13,   24,  -14]],\n",
      "\n",
      "         [[  23,    1,  -29],\n",
      "          [  23,   15,  -18],\n",
      "          [  12,  -14,   23]],\n",
      "\n",
      "         [[  24,   12,  -18],\n",
      "          [  25,  -19,  -23],\n",
      "          [   9,   14,   -9]]],\n",
      "\n",
      "\n",
      "        [[[   4,   51,   33],\n",
      "          [   4,   27,   32],\n",
      "          [  26,    9,  -13]],\n",
      "\n",
      "         [[ -22,  -17,   -2],\n",
      "          [  12,   13,    5],\n",
      "          [   2,   -2,  -19]],\n",
      "\n",
      "         [[ -42,  -16,  -34],\n",
      "          [ -13,  -44,    0],\n",
      "          [  33,   15,  -47]]],\n",
      "\n",
      "\n",
      "        [[[ -20,  -55,   10],\n",
      "          [  12,  -30,   12],\n",
      "          [  26,    1,   -4]],\n",
      "\n",
      "         [[   8,  -25,   25],\n",
      "          [ -21,   14,   19],\n",
      "          [ -22,    7,  -14]],\n",
      "\n",
      "         [[  13,   -3,   28],\n",
      "          [ -12,   30,   19],\n",
      "          [   4,   28,    5]]],\n",
      "\n",
      "\n",
      "        [[[   6,   66,    6],\n",
      "          [ -73,  -36,   17],\n",
      "          [  30,  -28,  -20]],\n",
      "\n",
      "         [[  27,   71,   23],\n",
      "          [ -82,  -34,   49],\n",
      "          [   6,  -58,   11]],\n",
      "\n",
      "         [[  26,   48,  -22],\n",
      "          [ -63,  -42,   58],\n",
      "          [  15,   12,  -26]]],\n",
      "\n",
      "\n",
      "        [[[ -30,   14,  -12],\n",
      "          [  44,   14,  -13],\n",
      "          [   8,   -4,   39]],\n",
      "\n",
      "         [[  14,  -56,   31],\n",
      "          [ -37,  -52,   31],\n",
      "          [  -6,  -45,   -5]],\n",
      "\n",
      "         [[  44,  -19,   22],\n",
      "          [  47,   -2,   20],\n",
      "          [  19,  -43,  -17]]],\n",
      "\n",
      "\n",
      "        [[[  24,   62,   10],\n",
      "          [  55,   13,  -77],\n",
      "          [ -18,  -21,  -53]],\n",
      "\n",
      "         [[ -29,   61,   31],\n",
      "          [  37,   -6,  -63],\n",
      "          [ -15,  -20,  -38]],\n",
      "\n",
      "         [[  -4,   42,  -13],\n",
      "          [   0,    8,   -8],\n",
      "          [  45,  -35,    1]]]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights Statistics:\\n\")\n",
    "print(torch.int_repr(cnn_quantized.conv[0].weight()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de9190",
   "metadata": {},
   "source": [
    "#### Int8 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2b2d315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc INT8     : 71.33%\n",
      "Latency INT8 : 2.80 ms/batch\n",
      "Size INT8    : 0.06 MB\n"
     ]
    }
   ],
   "source": [
    "acc_int8 = validate_model(cnn_quantized, val_loader, device, desc='Final INT8 Eval')\n",
    "lat_int8 = measure_inference_time(cnn_quantized, val_loader, device)\n",
    "model_size_int8 = get_model_size(cnn_quantized)\n",
    "\n",
    "print(f\"Acc INT8     : {acc_int8:.2f}%\")\n",
    "print(f\"Latency INT8 : {lat_int8:.2f} ms/batch\")\n",
    "print(f\"Size INT8    : {model_size_int8:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f8984",
   "metadata": {},
   "source": [
    "### Resumo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda45ab",
   "metadata": {},
   "source": [
    "Acho que até aqui fica claro o que está sendo feito. PTQ Estática com calibração MinMax. No experimento 2 quero explorar uma CNN mais robusta, mostrar o efeito do Fold BatchNormalization, o efeito da granularidade por tensor e por canal e o motivo da dificuldade relacionada aos modelos pré-treinados\n",
    "\n",
    "#### Sobre a fusão de camadas\n",
    "\n",
    "Durante a inferência, operações como **Conv2d → BatchNorm2d → ReLU** são executadas sequencialmente: cada operação lê da memória, computa e escreve de volta. A ideia de realizar uma *fusão de camadas* combina essas operações em um único kernel, eliminando as escritas/leituras intermediárias. Os efeitos principais são:\n",
    "\n",
    "- Latência menor — menos viagens à memória cache/RAM.  \n",
    "- Sem perda de acurácia — matematicamente equivalente ao pipeline original (em eval mode).  \n",
    "\n",
    "Ao fundir Conv+BN antes de quantizar, o quantizador vê uma distribuição de pesos mais estreita, melhorando a resolução INT8.\n",
    "\n",
    "Importante lembrar que a fusão só é possível em `model.eval()`. Em modo treino o BN ainda atualiza running stats, então a fusão seria incorreta.\n",
    "\n",
    "#### Fold BN (Batch Normalization Folding)\n",
    "\n",
    "Fluxo típico:\n",
    "\n",
    "1. Treinamento normal com\n",
    "- Conv → BatchNorm → ReLU\n",
    "2. Modelo entra em modo eval\n",
    "- (usa média e variância fixas da BN)\n",
    "3. Fold BatchNorm dentro da Convolution\n",
    "- A BN é incorporada nos pesos e bias da Conv\n",
    "- A camada BN é removida do grafo\n",
    "4. Quantização (PTQ ou QAT)\n",
    "- Agora o modelo já está como: Conv(folded) → ReLU\n",
    "- Pesos e ativações são quantizados\n",
    "\n",
    "A BatchNorm em modo inferência é só uma transformação linear, para cada canal de saída $c$:\n",
    "\n",
    "$$\n",
    "y_c = \\gamma_c \\cdot \\frac{x_c - \\mu_c}{\\sqrt{\\sigma_c^2 + \\varepsilon}} + \\beta_c\n",
    "$$\n",
    "\n",
    "onde:\n",
    "\n",
    "- $\\mu_c = \\text{running\\_mean}_c$\n",
    "- $\\sigma_c^2 = \\text{running\\_var}_c$\n",
    "\n",
    "Convolução antes do folding\n",
    "\n",
    "$$\n",
    "x_c = W_c * a + b_c\n",
    "$$\n",
    "\n",
    "onde:\n",
    "\n",
    "- $W_c$ = pesos do canal de saída $c$  \n",
    "- $b_c$ = bias do canal $c$  \n",
    "- $a$ = ativação de entrada  \n",
    "\n",
    "Substituindo Conv dentro da BN\n",
    "\n",
    "$$\n",
    "y_c =\n",
    "\\gamma_c\n",
    "\\frac{(W_c * a + b_c) - \\mu_c}{\\sqrt{\\sigma_c^2 + \\varepsilon}}\n",
    "+ \\beta_c\n",
    "$$\n",
    "\n",
    "Fatorando:\n",
    "\n",
    "$$\n",
    "y_c =\n",
    "\\left(\n",
    "\\frac{\\gamma_c}{\\sqrt{\\sigma_c^2 + \\varepsilon}}\n",
    "\\right)\n",
    "W_c * a\n",
    "+\n",
    "\\left(\n",
    "\\frac{\\gamma_c (b_c - \\mu_c)}{\\sqrt{\\sigma_c^2 + \\varepsilon}}\n",
    "+ \\beta_c\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Equações do folding\n",
    "\n",
    "Definindo:\n",
    "\n",
    "$$\n",
    "\\alpha_c = \\frac{\\gamma_c}{\\sqrt{\\sigma_c^2 + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "#### Pesos fundidos\n",
    "\n",
    "$$\n",
    "W'_c = \\alpha_c \\, W_c\n",
    "$$\n",
    "\n",
    "#### Bias fundido\n",
    "\n",
    "$$\n",
    "b'_c =\n",
    "\\alpha_c (b_c - \\mu_c)\n",
    "+ \\beta_c\n",
    "$$\n",
    "\n",
    "Forma vetorial (implementação prática)\n",
    "\n",
    "$$\n",
    "W' = W \\cdot \\frac{\\gamma}{\\sqrt{\\text{running\\_var} + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b' =\n",
    "\\left(\n",
    "b - \\text{running\\_mean}\n",
    "\\right)\n",
    "\\cdot\n",
    "\\frac{\\gamma}{\\sqrt{\\text{running\\_var} + \\varepsilon}}\n",
    "+ \\beta\n",
    "$$\n",
    "\n",
    "A multiplicação é por canal de saída (broadcasting).\n",
    "\n",
    "Caso a Conv não tenha bias, considere $b = 0$:\n",
    "\n",
    "$$\n",
    "b' =\n",
    "-\\mu \\cdot \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\varepsilon}}\n",
    "+ \\beta\n",
    "$$\n",
    "\n",
    "Após o folding, a camada BatchNorm desaparece e resta apenas uma Conv com pesos \\(W'\\) e bias \\(b'\\), pronta para quantização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503590f",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8daff85",
   "metadata": {},
   "source": [
    "#### CNN No Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc8b940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NO_OBS(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.fc   = nn.Linear(128 * 8 * 8, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c3c46",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f957804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training experiment2_cnn_fp32 | Epochs: 5 | LR: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 782/782 [00:30<00:00, 25.30it/s, loss=1.5273]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 1: Loss = 1.5273 | Acc = 64.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 782/782 [00:31<00:00, 24.71it/s, loss=1.0502]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 2: Loss = 1.0502 | Acc = 60.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 782/782 [00:31<00:00, 24.87it/s, loss=0.8913]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 3: Loss = 0.8913 | Acc = 71.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 782/782 [00:31<00:00, 24.69it/s, loss=0.7889]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 4: Loss = 0.7889 | Acc = 75.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 782/782 [00:31<00:00, 25.18it/s, loss=0.7156]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Epoch 5: Loss = 0.7156 | Acc = 76.63%\n",
      "Results:\n",
      "Acc     : 76.63%\n",
      "Latency : 8.63 ms/batch\n",
      "Size    : 0.67806 MB\n"
     ]
    }
   ],
   "source": [
    "cnn_fp32 = CNN_NO_OBS(num_classes=10).to(device)\n",
    "cnn_fp32 = train_model(\n",
    "    cnn_fp32, train_loader, val_loader, device,\n",
    "    epochs=5, lr=0.01, model_name=\"experiment2_cnn_fp32\"\n",
    ")\n",
    "\n",
    "acc_cnn_fp32  = evaluate_model(cnn_fp32, val_loader, device)\n",
    "time_cnn_fp32 = measure_inference_time(cnn_fp32, val_loader, device)\n",
    "size_cnn_fp32 = get_model_size(cnn_fp32)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"Acc     : {acc_cnn_fp32:.2f}%\")\n",
    "print(f\"Latency : {time_cnn_fp32:.2f} ms/batch\")\n",
    "print(f\"Size    : {size_cnn_fp32:.5f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d18566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Structure:\n",
      "conv1.weight -> torch.Size([32, 3, 3, 3])\n",
      "conv1.bias -> torch.Size([32])\n",
      "bn1.weight -> torch.Size([32])\n",
      "bn1.bias -> torch.Size([32])\n",
      "conv2.weight -> torch.Size([64, 32, 3, 3])\n",
      "conv2.bias -> torch.Size([64])\n",
      "bn2.weight -> torch.Size([64])\n",
      "bn2.bias -> torch.Size([64])\n",
      "conv3.weight -> torch.Size([128, 64, 3, 3])\n",
      "conv3.bias -> torch.Size([128])\n",
      "bn3.weight -> torch.Size([128])\n",
      "bn3.bias -> torch.Size([128])\n",
      "fc.weight -> torch.Size([10, 8192])\n",
      "fc.bias -> torch.Size([10])\n",
      "==========================================\n",
      "Weights Statistics:\n",
      "\n",
      "conv1.weight:\n",
      "tensor([ 0.0483,  0.0565,  0.1348, -0.1159, -0.1469, -0.1909,  0.1750, -0.2315,\n",
      "        -0.1932,  0.1487])\n",
      "Dtype: torch.float32\n",
      "Shape: (32, 3, 3, 3)\n",
      "Mean: 0.000287\n",
      "Std:  0.192709\n",
      "------------------------------------------\n",
      "conv1.bias:\n",
      "tensor([ 0.1551,  0.0933, -0.1757,  0.1327,  0.1873,  0.0394,  0.0653,  0.0367,\n",
      "        -0.0382, -0.1751])\n",
      "Dtype: torch.float32\n",
      "Shape: (32,)\n",
      "Mean: -0.006533\n",
      "Std:  0.101757\n",
      "------------------------------------------\n",
      "bn1.weight:\n",
      "tensor([0.9763, 0.9166, 0.9827, 1.0991, 1.0657, 1.0045, 0.9678, 0.9534, 1.1026,\n",
      "        1.0404])\n",
      "Dtype: torch.float32\n",
      "Shape: (32,)\n",
      "Mean: 0.971007\n",
      "Std:  0.111075\n",
      "------------------------------------------\n",
      "bn1.bias:\n",
      "tensor([-0.1910, -0.2197, -0.0236, -0.0390,  0.0472, -0.0615, -0.1265, -0.1738,\n",
      "        -0.0939, -0.0039])\n",
      "Dtype: torch.float32\n",
      "Shape: (32,)\n",
      "Mean: -0.053595\n",
      "Std:  0.096025\n",
      "------------------------------------------\n",
      "conv2.weight:\n",
      "tensor([ 0.0059,  0.0608, -0.0275, -0.0178,  0.0342, -0.0508,  0.0433, -0.0869,\n",
      "        -0.0165, -0.0177])\n",
      "Dtype: torch.float32\n",
      "Shape: (64, 32, 3, 3)\n",
      "Mean: -0.003041\n",
      "Std:  0.051218\n",
      "------------------------------------------\n",
      "conv2.bias:\n",
      "tensor([ 0.0014,  0.0367, -0.0503, -0.0392,  0.0283,  0.0521, -0.0307, -0.0346,\n",
      "         0.0453,  0.0364])\n",
      "Dtype: torch.float32\n",
      "Shape: (64,)\n",
      "Mean: 0.000668\n",
      "Std:  0.036872\n",
      "------------------------------------------\n",
      "bn2.weight:\n",
      "tensor([1.0016, 1.0619, 0.7925, 1.0951, 0.9664, 1.0141, 0.9558, 0.9338, 0.9893,\n",
      "        1.0850])\n",
      "Dtype: torch.float32\n",
      "Shape: (64,)\n",
      "Mean: 0.962564\n",
      "Std:  0.069442\n",
      "------------------------------------------\n",
      "bn2.bias:\n",
      "tensor([-0.0171, -0.1034, -0.1913, -0.1442, -0.0488, -0.2263, -0.0942, -0.1496,\n",
      "        -0.1625, -0.1524])\n",
      "Dtype: torch.float32\n",
      "Shape: (64,)\n",
      "Mean: -0.151326\n",
      "Std:  0.075503\n",
      "------------------------------------------\n",
      "conv3.weight:\n",
      "tensor([ 0.0238, -0.0200, -0.0222, -0.0026,  0.0050,  0.0035,  0.0148, -0.0194,\n",
      "         0.0269,  0.0421])\n",
      "Dtype: torch.float32\n",
      "Shape: (128, 64, 3, 3)\n",
      "Mean: -0.003644\n",
      "Std:  0.033593\n",
      "------------------------------------------\n",
      "conv3.bias:\n",
      "tensor([ 0.0310, -0.0387,  0.0333,  0.0262, -0.0338, -0.0264, -0.0059,  0.0294,\n",
      "         0.0138, -0.0145])\n",
      "Dtype: torch.float32\n",
      "Shape: (128,)\n",
      "Mean: 0.001653\n",
      "Std:  0.023705\n",
      "------------------------------------------\n",
      "bn3.weight:\n",
      "tensor([0.6776, 0.7597, 0.7166, 0.6994, 0.7081, 0.6781, 0.7132, 0.6524, 0.7221,\n",
      "        0.7563])\n",
      "Dtype: torch.float32\n",
      "Shape: (128,)\n",
      "Mean: 0.708780\n",
      "Std:  0.032015\n",
      "------------------------------------------\n",
      "bn3.bias:\n",
      "tensor([-0.4205, -0.4449, -0.4189, -0.4148, -0.3244, -0.4117, -0.4236, -0.4334,\n",
      "        -0.4438, -0.3993])\n",
      "Dtype: torch.float32\n",
      "Shape: (128,)\n",
      "Mean: -0.405964\n",
      "Std:  0.031751\n",
      "------------------------------------------\n",
      "fc.weight:\n",
      "tensor([ 0.0034,  0.0071, -0.0075, -0.0024,  0.0102,  0.0157, -0.0120,  0.0299,\n",
      "        -0.0083, -0.0059])\n",
      "Dtype: torch.float32\n",
      "Shape: (10, 8192)\n",
      "Mean: 0.000017\n",
      "Std:  0.027489\n",
      "------------------------------------------\n",
      "fc.bias:\n",
      "tensor([ 0.0438, -0.0652, -0.0484,  0.0350,  0.0400,  0.0213, -0.0386,  0.0190,\n",
      "        -0.0734,  0.0735])\n",
      "Dtype: torch.float32\n",
      "Shape: (10,)\n",
      "Mean: 0.000691\n",
      "Std:  0.052106\n",
      "------------------------------------------\n",
      "Datatype model:\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"best_experiment2_cnn_fp32.pth\", map_location=\"cpu\", weights_only=True)\n",
    "cnn_fp32.load_state_dict(state_dict)\n",
    "cnn_fp32.eval()\n",
    "\n",
    "print(\"Weights Structure:\")\n",
    "for name, param in cnn_fp32.named_parameters():\n",
    "    print(f\"{name} -> {param.shape}\")\n",
    "    \n",
    "print('='*42)\n",
    "\n",
    "print(\"Weights Statistics:\\n\")\n",
    "\n",
    "for name, param in cnn_fp32.named_parameters():\n",
    "    print(f\"{name}:\")\n",
    "    print(param.data.view(-1)[:10])\n",
    "    print(f\"Dtype: {param.dtype}\")\n",
    "    print(f\"Shape: {tuple(param.shape)}\")\n",
    "    print(f\"Mean: {param.data.mean().item():.6f}\")\n",
    "    print(f\"Std:  {param.data.std().item():.6f}\")\n",
    "    print('-'*42)\n",
    "  \n",
    "print('Datatype model:')  \n",
    "print(next(cnn_fp32.parameters()).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a4a452",
   "metadata": {},
   "source": [
    "#### CNN Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b6d3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_OBS(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.quant   = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.fc   = nn.Linear(128 * 8 * 8, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.quant(x)\n",
    "\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    CNN_FUSION_PATTERNS = [\n",
    "        [\"conv1\", \"bn1\", \"relu1\"],\n",
    "        [\"conv2\", \"bn2\", \"relu2\"],\n",
    "        [\"conv3\", \"bn3\", \"relu3\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de97498",
   "metadata": {},
   "source": [
    "#### Folding BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6533a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_conv_bn_relu_sequences(module: nn.Module) -> List[List[str]]:\n",
    "    children = list(module.named_children())\n",
    "    patterns: List[List[str]] = []\n",
    "    i = 0\n",
    "    while i < len(children):\n",
    "        n0, m0 = children[i]\n",
    "        if isinstance(m0, nn.Conv2d) and i + 1 < len(children):\n",
    "            n1, m1 = children[i + 1]\n",
    "            if isinstance(m1, nn.BatchNorm2d):\n",
    "                if i + 2 < len(children):\n",
    "                    n2, m2 = children[i + 2]\n",
    "                    if isinstance(m2, (nn.ReLU, nn.ReLU6)):\n",
    "                        patterns.append([n0, n1, n2])\n",
    "                        i += 3\n",
    "                        continue\n",
    "                patterns.append([n0, n1])\n",
    "                i += 2\n",
    "                continue\n",
    "        i += 1\n",
    "    return patterns\n",
    "\n",
    "def fuse_bn_recursively(model: nn.Module) -> nn.Module:\n",
    "    assert not model.training, \"Call model.eval() before BN folding\"\n",
    "    for _, child in model.named_children():\n",
    "        fuse_bn_recursively(child)\n",
    "    patterns = _find_conv_bn_relu_sequences(model)\n",
    "    if patterns:\n",
    "        torch.quantization.fuse_modules(model, patterns, inplace=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7410828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ptq(model: nn.Module, calibration_loader, mode: Literal[\"per_tensor\", \"per_channel\"] = \"per_channel\",\n",
    "            fold_bn: bool = False, num_calibration_batches: int = 10, custom_fusion_patterns: Optional[List[List[str]]] = None) -> nn.Module:\n",
    "    \n",
    "    backend = \"fbgemm\"\n",
    "    torch.backends.quantized.engine = backend\n",
    "\n",
    "    model = copy.deepcopy(model).cpu().eval()\n",
    "\n",
    "    if fold_bn:\n",
    "        if custom_fusion_patterns is not None:\n",
    "            torch.quantization.fuse_modules(\n",
    "                model, custom_fusion_patterns, inplace=True\n",
    "            )\n",
    "        else:\n",
    "            fuse_bn_recursively(model)\n",
    "\n",
    "    if mode == \"per_channel\":\n",
    "        qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "    else:\n",
    "        qconfig = torch.quantization.QConfig(\n",
    "            activation=torch.quantization.MinMaxObserver.with_args(\n",
    "                dtype=torch.quint8, qscheme=torch.per_tensor_affine\n",
    "            ),\n",
    "            weight=torch.quantization.MinMaxObserver.with_args(\n",
    "                dtype=torch.qint8, qscheme=torch.per_tensor_symmetric\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    model.qconfig = qconfig\n",
    "    model_prepared = torch.quantization.prepare(model, inplace=False)\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(calibration_loader):\n",
    "            if i >= num_calibration_batches:\n",
    "                break\n",
    "            model_prepared(data.cpu())\n",
    "\n",
    "    model_quantized = torch.quantization.convert(model_prepared, inplace=False)\n",
    "    return model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f80c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                              Acc     Latency      Size\n",
      "============================================================\n",
      "CNN FP32 without layer fusion   76.63%      8.63ms   0.678MB\n",
      "CNN FP32 with layer fusion      76.63%      7.86ms   0.671MB\n"
     ]
    }
   ],
   "source": [
    "cnn_fused = copy.deepcopy(cnn_fp32).cpu().eval()\n",
    "torch.quantization.fuse_modules(cnn_fused, CNN_OBS.CNN_FUSION_PATTERNS, inplace=True)\n",
    "\n",
    "lat_fused  = measure_inference_time(cnn_fused, val_loader, device)\n",
    "acc_fused  = evaluate_model(cnn_fused, val_loader, device)\n",
    "size_fused = get_model_size(cnn_fused)\n",
    "\n",
    "print(f\"{'Model':<30} {'Acc':>7}  {'Latency':>10}  {'Size':>8}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'CNN FP32 without layer fusion':<30} {acc_cnn_fp32:>6.2f}%  {time_cnn_fp32:>8.2f}ms  {size_cnn_fp32:>6.3f}MB\")\n",
    "print(f\"{'CNN FP32 with layer fusion':<30} {acc_fused:>6.2f}%  {lat_fused:>8.2f}ms  {size_fused:>6.3f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "\n",
    "cnn_quant_base = CNNOBS(num_classes=10)\n",
    "cnn_quant_base.load_state_dict(torch.load(\"best_cnn_fp32.pth\", map_location=\"cpu\", weights_only=True))\n",
    "\n",
    "print(\"Quantizando: per_tensor ...\")\n",
    "cnn_pt = apply_ptq(cnn_quant_base, val_loader, mode=\"per_tensor\",  fold_bn=False, custom_fusion_patterns=CNN.CNN_FUSION_PATTERNS)\n",
    "\n",
    "print(\"Quantizando: per_tensor + fold_bn ...\")\n",
    "cnn_pt_fold = apply_ptq(cnn_quant_base, val_loader, mode=\"per_tensor\",  fold_bn=True, custom_fusion_patterns=CNN.CNN_FUSION_PATTERNS)\n",
    "\n",
    "print(\"Quantizando: per_channel ...\")\n",
    "cnn_pc = apply_ptq(cnn_quant_base, val_loader, mode=\"per_channel\", fold_bn=False, custom_fusion_patterns=CNN.CNN_FUSION_PATTERNS)\n",
    "\n",
    "print(\"Quantizando: per_channel + fold_bn ...\")\n",
    "cnn_pc_fold = apply_ptq(cnn_quant_base, val_loader, mode=\"per_channel\", fold_bn=True, custom_fusion_patterns=CNN.CNN_FUSION_PATTERNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all(model, name):\n",
    "    acc  = evaluate_model(model, val_loader, device)\n",
    "    lat  = measure_inference_time(model, val_loader, device)\n",
    "    size = get_model_size(model)\n",
    "    print(f\"{name:<35} acc={acc:.2f}%  lat={lat:.2f}ms  size={size:.3f}MB\")\n",
    "    return acc, lat, size\n",
    "\n",
    "print(f\"{'Modelo':<35} {'Acc':>8}  {'Latência':>10}  {'Tamanho':>9}\")\n",
    "print(\"-\" * 70)\n",
    "acc_fp32_cnn,  lat_fp32_cnn,  size_fp32_cnn  = eval_all(cnn_fp32,      \"CNN FP32\")\n",
    "acc_pt,        lat_pt,        size_pt         = eval_all(cnn_pt,       \"CNN per_tensor\")\n",
    "acc_pt_fold,   lat_pt_fold,   size_pt_fold    = eval_all(cnn_pt_fold,  \"CNN per_tensor + fold_bn\")\n",
    "acc_pc,        lat_pc,        size_pc         = eval_all(cnn_pc,       \"CNN per_channel\")\n",
    "acc_pc_fold,   lat_pc_fold,   size_pc_fold    = eval_all(cnn_pc_fold,  \"CNN per_channel + fold_bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std =[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Recria o val loader com transforms de ImageNet para avaliação dos modelos pré-treinados\n",
    "import torchvision.datasets as dsets\n",
    "cifar10_imagenet_val = dsets.CIFAR10(\n",
    "    \"../data\", train=False, download=True, transform=imagenet_transform\n",
    ")\n",
    "imagenet_val_loader = torch.utils.data.DataLoader(\n",
    "    cifar10_imagenet_val, batch_size=64, shuffle=False, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "def evaluate_pretrained_variants(model_name: str):\n",
    "    \"\"\"\n",
    "    Carrega, quantiza e avalia um modelo pré-treinado nas 4 variantes PTQ.\n",
    "    Retorna dict com acc, lat, size para fp32 e cada variante.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Modelo: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    torch.backends.quantized.engine = \"fbgemm\"\n",
    "\n",
    "    # FP32 baseline\n",
    "    model_fp32 = prepare_pretrained_model(model_name, num_classes=1000, wrap=False)\n",
    "    model_fp32.eval().cpu()\n",
    "\n",
    "    acc_fp32_ = evaluate_model(model_fp32, imagenet_val_loader, device)\n",
    "    lat_fp32_ = measure_inference_time(model_fp32, imagenet_val_loader, device, num_batches=10)\n",
    "    sz_fp32_  = get_model_size(model_fp32)\n",
    "    print(f\"  FP32       acc={acc_fp32_:.2f}%  lat={lat_fp32_:.1f}ms  size={sz_fp32_:.1f}MB\")\n",
    "\n",
    "    # Para quantização precisamos do wrapper\n",
    "    base_wrapped = prepare_pretrained_model(model_name, num_classes=1000, wrap=True)\n",
    "\n",
    "    variants = {\n",
    "        \"per_tensor\":         dict(mode=\"per_tensor\",  fold_bn=False),\n",
    "        \"per_tensor_foldbn\":  dict(mode=\"per_tensor\",  fold_bn=True),\n",
    "        \"per_channel\":        dict(mode=\"per_channel\", fold_bn=False),\n",
    "        \"per_channel_foldbn\": dict(mode=\"per_channel\", fold_bn=True),\n",
    "    }\n",
    "\n",
    "    row = {\n",
    "        \"model\": model_name,\n",
    "        \"fp32_acc\": acc_fp32_, \"fp32_lat\": lat_fp32_, \"fp32_size\": sz_fp32_,\n",
    "    }\n",
    "\n",
    "    for tag, kwargs in variants.items():\n",
    "        q_model = apply_ptq(base_wrapped, imagenet_val_loader,\n",
    "                            num_calibration_batches=10, **kwargs)\n",
    "        acc_ = evaluate_model(q_model, imagenet_val_loader, device)\n",
    "        lat_ = measure_inference_time(q_model, imagenet_val_loader, device, num_batches=10)\n",
    "        sz_  = get_model_size(q_model)\n",
    "        print(f\"  {tag:<22} acc={acc_:.2f}%  lat={lat_:.1f}ms  size={sz_:.1f}MB\")\n",
    "        row[f\"{tag}_acc\"]  = acc_\n",
    "        row[f\"{tag}_lat\"]  = lat_\n",
    "        row[f\"{tag}_size\"] = sz_\n",
    "\n",
    "    return row\n",
    "\n",
    "# ── Executar para todos os modelos pré-treinados ──────────────────────────────\n",
    "pretrained_names = [\"mobilenet_v2\", \"resnet50\", \"resnext50_32x4d\", \"efficientnet_b0\"]\n",
    "pretrained_results = []\n",
    "\n",
    "for name in pretrained_names:\n",
    "    row = evaluate_pretrained_variants(name)\n",
    "    pretrained_results.append(row)\n",
    "\n",
    "print(\"\\nQuantização de modelos pré-treinados concluída!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# ── Tabela 1: Modelo | Variante | Latência (ms) | Tamanho (MB) ────────────────\n",
    "rows_t1 = []\n",
    "\n",
    "# CNN (custom)\n",
    "cnn_variants_t1 = [\n",
    "    (\"CNN\",  \"FP32\",                   lat_fp32_cnn,  size_fp32_cnn),\n",
    "    (\"CNN\",  \"per_tensor\",             lat_pt,        size_pt),\n",
    "    (\"CNN\",  \"per_tensor + fold_bn\",   lat_pt_fold,   size_pt_fold),\n",
    "    (\"CNN\",  \"per_channel\",            lat_pc,        size_pc),\n",
    "    (\"CNN\",  \"per_channel + fold_bn\",  lat_pc_fold,   size_pc_fold),\n",
    "]\n",
    "rows_t1.extend(cnn_variants_t1)\n",
    "\n",
    "# Modelos pré-treinados\n",
    "for r in pretrained_results:\n",
    "    m = r[\"model\"]\n",
    "    rows_t1.append((m, \"FP32\",                   r[\"fp32_lat\"],                r[\"fp32_size\"]))\n",
    "    rows_t1.append((m, \"per_tensor\",             r[\"per_tensor_lat\"],          r[\"per_tensor_size\"]))\n",
    "    rows_t1.append((m, \"per_tensor + fold_bn\",   r[\"per_tensor_foldbn_lat\"],   r[\"per_tensor_foldbn_size\"]))\n",
    "    rows_t1.append((m, \"per_channel\",            r[\"per_channel_lat\"],         r[\"per_channel_size\"]))\n",
    "    rows_t1.append((m, \"per_channel + fold_bn\",  r[\"per_channel_foldbn_lat\"],  r[\"per_channel_foldbn_size\"]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"TABELA 1 — Latência e Tamanho dos Modelos\")\n",
    "print(\"=\"*65)\n",
    "print(tabulate(\n",
    "    [[m, v, f\"{lat:.2f}\", f\"{sz:.3f}\"] for m, v, lat, sz in rows_t1],\n",
    "    headers=[\"Modelo\", \"Variante\", \"Latência (ms/batch)\", \"Tamanho (MB)\"],\n",
    "    tablefmt=\"fancy_grid\"\n",
    "))\n",
    "\n",
    "# ── Tabela 2: Modelo | FP32 | per_tensor | per_tensor_fold | per_channel | per_channel_fold ──\n",
    "rows_t2 = []\n",
    "\n",
    "# CNN\n",
    "rows_t2.append([\n",
    "    \"CNN\",\n",
    "    f\"{acc_fp32_cnn:.2f}\",\n",
    "    f\"{acc_pt:.2f}\",\n",
    "    f\"{acc_pt_fold:.2f}\",\n",
    "    f\"{acc_pc:.2f}\",\n",
    "    f\"{acc_pc_fold:.2f}\",\n",
    "])\n",
    "\n",
    "# Pré-treinados\n",
    "for r in pretrained_results:\n",
    "    rows_t2.append([\n",
    "        r[\"model\"],\n",
    "        f\"{r['fp32_acc']:.2f}\",\n",
    "        f\"{r['per_tensor_acc']:.2f}\",\n",
    "        f\"{r['per_tensor_foldbn_acc']:.2f}\",\n",
    "        f\"{r['per_channel_acc']:.2f}\",\n",
    "        f\"{r['per_channel_foldbn_acc']:.2f}\",\n",
    "    ])\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"TABELA 2 — Acurácia (%) por Estratégia de Quantização\")\n",
    "print(\"=\"*90)\n",
    "print(tabulate(\n",
    "    rows_t2,\n",
    "    headers=[\n",
    "        \"Modelo\", \"FP32 (%)\", \"per_tensor (%)\",\n",
    "        \"per_tensor\\n+fold_bn (%)\", \"per_channel (%)\",\n",
    "        \"per_channel\\n+fold_bn (%)\"\n",
    "    ],\n",
    "    tablefmt=\"fancy_grid\"\n",
    "))\n",
    "\n",
    "print(\"\\n[Legenda]\")\n",
    "print(\"  fold_bn   = BatchNorm absorvido nos pesos Conv antes da quantização\")\n",
    "print(\"  per_tensor = uma escala por tensor inteiro (pesos + ativações)\")\n",
    "print(\"  per_channel= uma escala por canal de saída (pesos) — padrão fbgemm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee97bb2",
   "metadata": {},
   "source": [
    "#### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizableWrapper(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.quant   = QuantStub()\n",
    "        self.model   = model\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def prepare_pretrained_model(model_name: str, num_classes: int = 1000, wrap: bool = True) -> nn.Module:\n",
    "    weights_map = {\n",
    "        \"mobilenet_v2\":      (models.mobilenet_v2,       models.MobileNet_V2_Weights.IMAGENET1K_V2),\n",
    "        \"mobilenet_v3_small\":(models.mobilenet_v3_small, models.MobileNet_V3_Small_Weights.IMAGENET1K_V1),\n",
    "        \"efficientnet_b0\":   (models.efficientnet_b0,    models.EfficientNet_B0_Weights.IMAGENET1K_V1),\n",
    "        \"resnet50\":          (models.resnet50,           models.ResNet50_Weights.IMAGENET1K_V2),\n",
    "        \"resnext50_32x4d\":   (models.resnext50_32x4d,   models.ResNeXt50_32X4D_Weights.IMAGENET1K_V2),\n",
    "    }\n",
    "\n",
    "    if model_name not in weights_map:\n",
    "        raise ValueError(f\"Model '{model_name}' not supported. Choose from: {list(weights_map)}\")\n",
    "\n",
    "    constructor, weights = weights_map[model_name]\n",
    "    model = constructor(weights=weights)\n",
    "\n",
    "    if num_classes != 1000:\n",
    "        if model_name == \"mobilenet_v2\":\n",
    "            model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "        elif model_name == \"mobilenet_v3_small\":\n",
    "            model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "        elif model_name == \"efficientnet_b0\":\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        elif model_name in (\"resnet50\", \"resnext50_32x4d\"):\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    return QuantizableWrapper(model) if wrap else model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
